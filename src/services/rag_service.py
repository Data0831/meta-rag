# src/services/rag_service.py
import re
import json
from typing import Dict, Any, List, Optional
from src.services.search_service import SearchService
from src.llm.client import LLMClient
from src.llm.rag_prompts import RAG_SYSTEM_PROMPT


class RAGService:
    def __init__(self):
        # åˆå§‹åŒ–ä¾è³´çš„æœå‹™
        self.search_service = SearchService()
        self.llm_client = LLMClient()

    def chat(
        self,
        user_query: str,
        provided_context: List[Dict] = None,
        history: List[Dict] = None,
    ) -> Dict[str, Any]:
        """
        RAG èŠå¤©ä¸»é‚è¼¯ (æœ€çµ‚å®Œæ•´ç‰ˆ)
        åŒ…å«ï¼š
        1. Context çµ„è£
        2. XML å»ºè­°å•é¡Œè§£æ (<suggestions><question>...</question></suggestions>)
        3. JSON æ ¼å¼å‚™æ´
        4. é»‘åå–®èˆ‡å“è³ªéæ¿¾
        5. ç§»é™¤ã€Œä»¥ä¸‹æ˜¯å»ºè­°å•é¡Œã€ç­‰é–‹å ´ç™½
        """
        print(f"RAGService: Processing query '{user_query}'")

        # --- æ­¥é©Ÿ 1: æ±ºå®š Context (è³‡æ–™ä¾†æº) ---
        results = []
        source_type = "search"

        if provided_context is not None:
            results = provided_context
            source_type = "provided"
        else:
            # åªæœ‰ç•¶ provided_context çœŸçš„æ˜¯ None (å‰ç«¯æ²’å‚³é€™å€‹æ¬„ä½) æ™‚
            # æ‰åŸ·è¡Œå¾Œç«¯çš„è‡ªå‹•è£œä½æœå°‹
            try:
                print("   No context provided, performing backend search...")
                search_data = self.search_service.search(
                    user_query=user_query, limit=3, semantic_ratio=0.5, enable_llm=True
                )
                results = search_data.get("results", [])
            except Exception as e:
                print(f"  Search failed: {e}")
                results = []

        # --- æ­¥é©Ÿ 2: å°‡æ–‡ä»¶çµ„è£æˆæ–‡å­—å­—ä¸² ---
        context_text = ""
        if results:
            for idx, doc in enumerate(results, 1):
                title = doc.get("title", "No Title")
                # è™•ç†å…§å®¹å¯èƒ½ç‚º None çš„æƒ…æ³
                raw_content = doc.get("content") or doc.get("cleaned_content") or ""
                content = str(raw_content)
                date = doc.get("year_month", "N/A")
                
                # æˆªæ–·éé•·å…§å®¹ä»¥ç¯€çœ Token
                if len(content) > 15000:
                    content = content[:15000] + "..."     
                context_text += f"Document {idx}:\nTitle: {title}\nDate: {date}\nContent: {content}\n\n"
        else:
            # è‹¥ç„¡æœå°‹çµæœï¼Œç›´æ¥å›å‚³ï¼Œä¸æµªè²» LLM è³‡æº
            return {
                "answer": "æ ¹æ“šç›®å‰çš„æœå°‹è¨­å®šï¼ˆç›¸ä¼¼åº¦é–€æª»ï¼‰ï¼Œæ‰¾ä¸åˆ°ç›¸é—œå…¬å‘Šå¯ä¾›å›ç­”ã€‚",
                "suggestions": ["å¦‚ä½•ä½¿ç”¨æœå°‹ï¼Ÿ", "æœ€è¿‘æœ‰ä»€éº¼å…¬å‘Šï¼Ÿ", "Copilot åƒ¹æ ¼æŸ¥è©¢"],
                "references": [],
            }

        # --- æ­¥é©Ÿ 3: çµ„è£ Messages ---
        messages = []
        # æ³¨æ„ï¼šè«‹ç¢ºèªä½ çš„ RAG_SYSTEM_PROMPT å·²ç¶“æ›´æ–°ç‚ºè¦æ±‚ XML æ ¼å¼
        full_system_prompt = RAG_SYSTEM_PROMPT.format(context=context_text)
        messages.append({"role": "system", "content": full_system_prompt})

        if history:
            for msg in history:
                role = "assistant" if msg.get("role") == "model" else "user"
                content = msg.get("content", "")
                if content:
                    messages.append({"role": role, "content": content})

        messages.append({"role": "user", "content": user_query})

        # --- æ­¥é©Ÿ 4: å‘¼å« LLM ---
        print("   Asking LLM...")
        answer_text = ""
        suggestions = []

        try:
            # é€™è£¡å‘¼å«ä½ çš„ LLM Client
            full_response = self.llm_client.call_gemini(messages=messages, temperature=0.1)
            answer_text = full_response
            
            # --- ğŸ”¥ğŸ”¥ğŸ”¥ã€è§£æèˆ‡æ¸…æ´—æ ¸å¿ƒé‚è¼¯ã€‘ğŸ”¥ğŸ”¥ğŸ”¥ ---

            # A. å„ªå…ˆè™•ç† XML <suggestions> 
            # ä½¿ç”¨ re.DOTALL è®“ . å¯ä»¥åŒ¹é…æ›è¡Œç¬¦è™Ÿ
            suggestion_match = re.search(r"<suggestions>(.*?)</suggestions>", full_response, re.DOTALL)
            
            if suggestion_match:
                xml_content = suggestion_match.group(1).strip()
                
                # å„ªåŒ– Regexï¼š
                # 1. å…è¨±æ¨™ç±¤å‰å¾Œæœ‰ç©ºç™½ (\s*)
                # 2. å¿½ç•¥å¤§å°å¯« (re.IGNORECASE)ï¼ŒæŠ“å– <Question> æˆ– <question>
                xml_questions = re.findall(r'<\s*question\s*>(.*?)<\s*/\s*question\s*>', xml_content, re.DOTALL | re.IGNORECASE)
                
                if xml_questions:
                    suggestions = [q.strip() for q in xml_questions]
                
                # åˆ‡å‰²ï¼šå°‡æ•´å€‹ <suggestions> å€å¡Šå¾å›ç­”ä¸­ç§»é™¤
                answer_text = answer_text.replace(suggestion_match.group(0), "")

            # B. å‚™æ´è™•ç† JSON List (ä»¥é˜² LLM å¶çˆ¾é‚„æ˜¯å JSON)
            # å¦‚æœä¸Šé¢ XML æ²’æŠ“åˆ°æ±è¥¿ï¼Œæ‰è·‘é€™æ®µ
            if not suggestions:
                json_array_pattern = r"\[\s*\"(?:\\.|[^\"\\])*\"(?:,\s*\"(?:\\.|[^\"\\])*\")*\s*\]"
                matches = list(re.finditer(json_array_pattern, full_response, re.DOTALL))
                
                for match in reversed(matches):
                    json_str = match.group(0)
                    try:
                        parsed = json.loads(json_str)
                        if isinstance(parsed, list):
                            suggestions = parsed
                            # ä½¿ç”¨ç´¢å¼•åˆ‡å‰² (Slicing) ç§»é™¤ JSON åŠå…¶å¾Œçš„æ‰€æœ‰å…§å®¹
                            cutoff_index = match.start()
                            answer_text = full_response[:cutoff_index]
                            break 
                    except:
                        continue

            # C. æ®˜éª¸æƒé™¤ (ç§»é™¤ Markdown æ¨™è¨˜)
            # ç§»é™¤çµå°¾å¯èƒ½çš„ ```xml, ```json, ```
            answer_text = re.sub(r"```\w*\s*$", "", answer_text.strip(), flags=re.IGNORECASE)
            answer_text = answer_text.replace("```", "").strip()

            # D. ğŸ”¥ å¼·åŠ›æ¸…æ´—èˆ‡éæ¿¾ ğŸ”¥
            if suggestions:
                final_clean_suggestions = []
                # é»‘åå–®ï¼šéæ¿¾æ‰ç³»çµ±é—œéµå­—æˆ–ç„¡æ„ç¾©çš„è©
                block_list = ["xml", "json", "question", "suggestions", "item", "none", "null", "nan", "[]", "list"]
                
                for s in suggestions:
                    # 1. ç§»é™¤å¯èƒ½æ®˜ç•™çš„ HTML/XML æ¨™ç±¤
                    s = re.sub(r'<[^>]+>', '', str(s)).strip()
                    
                    # 2. éæ¿¾æ¢ä»¶ï¼š
                    # - ä¸æ˜¯ç©ºå­—ä¸²
                    # - é•·åº¦ > 4 (é¿å…éçŸ­çš„ç„¡æ„ç¾©å­—ä¸²)
                    # - ä¸åœ¨é»‘åå–®å…§
                    if (s and len(s) > 4 and s.lower() not in block_list):
                        final_clean_suggestions.append(s)
                
                suggestions = final_clean_suggestions

                # E. ğŸ”¥ ç§»é™¤å›ç­”å°¾éƒ¨çš„ã€Œé–‹å ´ç™½ã€ ğŸ”¥
                # é¿å…æ©Ÿå™¨äººèªªå®Œã€Œä»¥ä¸‹æ˜¯å»ºè­°å•é¡Œï¼šã€çµæœå¾Œé¢æ˜¯ä¸€ç‰‡ç©ºç™½(å› ç‚ºè¢«æˆ‘å€‘åˆ‡æ‰äº†)
                removals = [
                    "ä»¥ä¸‹æ˜¯æ ¹æ“šæœå°‹çµæœï¼Œæ‚¨å¯èƒ½æ„Ÿèˆˆè¶£çš„å¾ŒçºŒå•é¡Œï¼š",
                    "æ‚¨å¯èƒ½æ„Ÿèˆˆè¶£çš„å¾ŒçºŒå•é¡Œï¼š",
                    "ç›¸é—œå»ºè­°å•é¡Œï¼š",
                    "å¾ŒçºŒå•é¡Œå»ºè­°ï¼š",
                    "Suggested questions:",
                    "Follow-up questions:"
                ]
                
                for pattern in removals:
                    answer_text = answer_text.replace(pattern, "")
                
                # å†æ¬¡ä¿®å‰ªå°¾éƒ¨çš„å†’è™Ÿæˆ–ç©ºç™½
                answer_text = answer_text.strip().rstrip("ï¼š:").strip()

            # F. ä¿åº•é‚è¼¯ (è‹¥å›ç­”è¢«åˆ‡å…‰å…‰ï¼Œçµ¦å€‹é è¨­å€¼)
            if not answer_text.strip():
                if suggestions:
                    answer_text = "æˆ‘å·²æ ¹æ“šæœå°‹çµæœæ•´ç†å‡ºå›ç­”ï¼Œè«‹åƒè€ƒä¸‹æ–¹çš„å»ºè­°å•é¡Œï¼š"
                else:
                    answer_text = "æŠ±æ­‰ï¼Œç³»çµ±æš«æ™‚ç„¡æ³•ç”Ÿæˆå®Œæ•´å›æ‡‰ã€‚"

        except Exception as e:
            print(f"âŒ LLM Error: {e}")
            answer_text = "æŠ±æ­‰ï¼ŒAI æœå‹™é€£ç·šç™¼ç”ŸéŒ¯èª¤ã€‚"
            suggestions = ["é‡æ–°æ•´ç†", "æª¢æŸ¥ç¶²è·¯", "é‡è©¦"]

        return {
            "answer": answer_text,
            "suggestions": suggestions,
            "references": results if source_type == "search" else [],
        }
    
    def summarize(self, user_query: str, search_results: List[Dict]) -> str:
        """
        é‡å°æœå°‹çµæœç”Ÿæˆæ‘˜è¦
        """
        print(f"ğŸ“ RAGService: Generating summary for '{user_query}'")

        if not search_results:
            return ""

        # 1. æº–å‚™ Context (åªå–å‰ 5 ç­†ï¼Œé¿å… Token å¤ªå¤š)
        context_text = ""
        for idx, doc in enumerate(search_results[:5], 1):
            title = doc.get("title", "No Title")
            content = doc.get("content", "") or doc.get("cleaned_content", "")
            # æ‘˜è¦åªéœ€è¦éƒ¨åˆ†å…§å®¹å³å¯
            if len(content) > 500:
                content = content[:500] + "..."
            context_text += f"[ç¬¬ {idx} ç¯‡] æ¨™é¡Œ: {title}\nå…§å®¹: {content}\n\n"

        # 2. çµ„è£ Prompt
        from src.llm.rag_prompts import SUMMARY_SYSTEM_PROMPT

        prompt = SUMMARY_SYSTEM_PROMPT.format(context=context_text, query=user_query)

        messages = [{"role": "user", "content": prompt}]

        # 3. å‘¼å« LLM
        try:
            # ä½¿ç”¨è¼ƒä½çš„ temperature (0.3) è®“æ‘˜è¦æ›´ç©©å®š
            summary = self.llm_client.call_gemini(messages=messages, temperature=0.3)
            return summary
        except Exception as e:
            print(f"âŒ Summary Generation Error: {e}")
            return ""

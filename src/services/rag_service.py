# src/services/rag_service.py

from typing import Dict, Any, List, Optional
from src.services.search_service import SearchService
from src.llm.client import LLMClient
from src.llm.rag_prompts import RAG_SYSTEM_PROMPT

class RAGService:
    def __init__(self):
        # åˆå§‹åŒ–ä¾è³´çš„æœå‹™
        self.search_service = SearchService()
        self.llm_client = LLMClient()

    def chat(self, user_query: str, provided_context: List[Dict] = None, history: List[Dict] = None) -> Dict[str, Any]:
        """
        RAG èŠå¤©ä¸»é‚è¼¯
        Args:
            user_query: ä½¿ç”¨è€…ç•¶å‰çš„å•é¡Œ
            provided_context: å‰ç«¯å‚³ä¾†çš„æœå°‹çµæœ (è‹¥æœ‰å‰‡å„ªå…ˆä½¿ç”¨)
            history: å‰ç«¯å‚³ä¾†çš„å°è©±æ­·å²ç´€éŒ„ [{role: 'user'/'model', content: '...'}]
        """
        
        print(f"ğŸ¤– RAGService: Processing query '{user_query}'")
        
        # --- æ­¥é©Ÿ 1: æ±ºå®š Context (è³‡æ–™ä¾†æº) ---
        results = []
        source_type = "search"

        if provided_context and len(provided_context) > 0:
            # A. å„ªå…ˆä½¿ç”¨å‰ç«¯å‚³ä¾†çš„æœå°‹çµæœ (Context Injection)
            print(f"  Using {len(provided_context)} documents provided by frontend.")
            results = provided_context
            source_type = "provided"
        else:
            # B. å¦‚æœå‰ç«¯æ²’å‚³ï¼Œå‰‡è‡ªå·±å»è³‡æ–™åº«æœå°‹ (Fallback)
            print(f"  No context provided, searching DB...")
            search_data = self.search_service.search(
                user_query=user_query,
                limit=3,
                semantic_ratio=0.5,
                enable_llm=True
            )
            results = search_data.get("results", [])

        # --- æ­¥é©Ÿ 2: å°‡æ–‡ä»¶çµ„è£æˆæ–‡å­—å­—ä¸² ---
        context_text = ""
        if results:
            for idx, doc in enumerate(results, 1):
                title = doc.get('title', 'No Title')
                content = doc.get('content', '') or doc.get('cleaned_content', '')
                date = doc.get('year_month', 'N/A')
                
                # ç°¡å–®æˆªæ–·éé•·çš„å…§å®¹ (é¿å…è¶…é Token é™åˆ¶)
                if len(content) > 800:
                    content = content[:800] + "..."
                
                context_text += f"Document {idx}:\nTitle: {title}\nDate: {date}\nContent: {content}\n\n"
        else:
            # å®Œå…¨ç„¡è³‡æ–™æ™‚çš„è™•ç†
            return {
                "answer": "æŠ±æ­‰ï¼Œç›®å‰æ²’æœ‰ç›¸é—œçš„æœå°‹çµæœå¯ä¾›åƒè€ƒï¼Œè«‹å˜—è©¦å…ˆåœ¨å·¦å´æœå°‹æ¬„è¼¸å…¥é—œéµå­—ã€‚",
                "references": []
            }

        # --- æ­¥é©Ÿ 3: çµ„è£ LLM çš„ Messages (åŒ…å« System Prompt + History) ---
        messages = []

        # (A) System Prompt: æ³¨å…¥ç•¶å‰çš„ Context
        full_system_prompt = RAG_SYSTEM_PROMPT.format(context=context_text)
        messages.append({"role": "system", "content": full_system_prompt})

        # (B) History: æ³¨å…¥æ­·å²ç´€éŒ„ (è®“ LLM æ“æœ‰çŸ­æœŸè¨˜æ†¶)
        if history:
            print(f"  Loading {len(history)} history messages...")
            for msg in history:
                # è½‰æ› role åç¨±: å‰ç«¯å‚³ä¾†çš„ 'model' å°æ‡‰ OpenAI çš„ 'assistant'
                role = "assistant" if msg.get("role") == "model" else "user"
                content = msg.get("content", "")
                if content:
                    messages.append({"role": role, "content": content})

        # (C) User Query: åŠ å…¥ç•¶å‰æœ€æ–°çš„å•é¡Œ
        messages.append({"role": "user", "content": user_query})

        # --- æ­¥é©Ÿ 4: å‘¼å« LLM ç”Ÿæˆå›ç­” ---
        print("  Asking LLM...")
        try:
            # â˜…â˜…â˜… ä¿®æ”¹é€™è£¡ï¼šå°‡ temperature å¾ 0.3 æ”¹æˆ 0.5 â˜…â˜…â˜…
            answer = self.llm_client.call_gemini(messages=messages, temperature=0.5)
            
            if not answer:
                answer = "æŠ±æ­‰ï¼Œç³»çµ±æš«æ™‚ç„¡æ³•ç”Ÿæˆå›æ‡‰ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"
                
        except Exception as e:
            print(f"âŒ LLM Error: {e}")
            answer = "æŠ±æ­‰ï¼ŒAI æœå‹™é€£ç·šç™¼ç”ŸéŒ¯èª¤ã€‚"

        return {
            "answer": answer,
            "references": results if source_type == "search" else [] # å¦‚æœæ˜¯å‰ç«¯å‚³çš„ï¼Œé€šå¸¸ä¸éœ€è¦å†å›å‚³ references
        }
    
    def summarize(self, user_query: str, search_results: List[Dict]) -> str:
        """
        é‡å°æœå°‹çµæœç”Ÿæˆæ‘˜è¦
        """
        print(f"ğŸ“ RAGService: Generating summary for '{user_query}'")
        
        if not search_results:
            return ""

        # 1. æº–å‚™ Context (åªå–å‰ 5 ç­†ï¼Œé¿å… Token å¤ªå¤š)
        context_text = ""
        for idx, doc in enumerate(search_results[:5], 1):
            title = doc.get('title', 'No Title')
            content = doc.get('content', '') or doc.get('cleaned_content', '')
            # æ‘˜è¦åªéœ€è¦éƒ¨åˆ†å…§å®¹å³å¯
            if len(content) > 500:
                content = content[:500] + "..."
            context_text += f"[ç¬¬ {idx} ç¯‡] æ¨™é¡Œ: {title}\nå…§å®¹: {content}\n\n"

        # 2. çµ„è£ Prompt
        from src.llm.rag_prompts import SUMMARY_SYSTEM_PROMPT
        prompt = SUMMARY_SYSTEM_PROMPT.format(context=context_text, query=user_query)
        
        messages = [{"role": "user", "content": prompt}]

        # 3. å‘¼å« LLM
        try:
            # ä½¿ç”¨è¼ƒä½çš„ temperature (0.3) è®“æ‘˜è¦æ›´ç©©å®š
            summary = self.llm_client.call_gemini(messages=messages, temperature=0.3)
            return summary
        except Exception as e:
            print(f"âŒ Summary Generation Error: {e}")
            return ""
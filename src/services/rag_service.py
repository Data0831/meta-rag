# src/services/rag_service.py
import re
import json
from typing import Dict, Any, List, Optional
from src.services.search_service import SearchService
from src.llm.client import LLMClient
from src.llm.rag_prompts import RAG_SYSTEM_PROMPT

class RAGService:
    def __init__(self):
        # åˆå§‹åŒ–ä¾è³´çš„æœå‹™
        self.search_service = SearchService()
        self.llm_client = LLMClient()

    def chat(self, user_query: str, provided_context: List[Dict] = None, history: List[Dict] = None) -> Dict[str, Any]:
        """
        RAG èŠå¤©ä¸»é‚è¼¯
        """
        print(f"ğŸ¤– RAGService: Processing query '{user_query}'")
        
        # --- æ­¥é©Ÿ 1: æ±ºå®š Context (è³‡æ–™ä¾†æº) ---
        results = []
        source_type = "search"

        if provided_context and len(provided_context) > 0:
            print(f"  Using {len(provided_context)} documents provided by frontend.")
            results = provided_context
            source_type = "provided"
        else:
            print(f"  No context provided, searching DB...")
            try:
                search_data = self.search_service.search(
                    user_query=user_query,
                    limit=3,
                    semantic_ratio=0.5,
                    enable_llm=True
                )
                results = search_data.get("results", [])
            except Exception as e:
                print(f"  Search failed: {e}")
                results = []

        # --- æ­¥é©Ÿ 2: å°‡æ–‡ä»¶çµ„è£æˆæ–‡å­—å­—ä¸² ---
        context_text = ""
        if results:
            for idx, doc in enumerate(results, 1):
                title = doc.get('title', 'No Title')
                
                # â˜…â˜…â˜… ä¿®å¾©é‡é» 1: å¼·åˆ¶è½‰å­—ä¸²ï¼Œé˜²æ­¢ None å°è‡´ crash â˜…â˜…â˜…
                raw_content = doc.get('content') or doc.get('cleaned_content') or ""
                content = str(raw_content) 

                date = doc.get('year_month', 'N/A')
                
                if len(content) > 800:
                    content = content[:800] + "..."
                
                context_text += f"Document {idx}:\nTitle: {title}\nDate: {date}\nContent: {content}\n\n"
        else:
            # è‹¥ç„¡è³‡æ–™ï¼Œå›å‚³æç¤ºèˆ‡å¼•å°æŒ‰éˆ•
            return {
                "answer": "æŠ±æ­‰ï¼Œç›®å‰æ²’æœ‰ç›¸é—œçš„æœå°‹çµæœå¯ä¾›åƒè€ƒï¼Œè«‹å˜—è©¦å…ˆåœ¨å·¦å´æœå°‹æ¬„è¼¸å…¥é—œéµå­—ã€‚",
                "suggestions": ["å¦‚ä½•ä½¿ç”¨æœå°‹ï¼Ÿ", "æœ€è¿‘æœ‰ä»€éº¼å…¬å‘Šï¼Ÿ", "Copilot åƒ¹æ ¼æŸ¥è©¢"],
                "references": []
            }

        # --- æ­¥é©Ÿ 3: çµ„è£ LLM çš„ Messages ---
        messages = []
        full_system_prompt = RAG_SYSTEM_PROMPT.format(context=context_text)
        messages.append({"role": "system", "content": full_system_prompt})

        if history:
            for msg in history:
                role = "assistant" if msg.get("role") == "model" else "user"
                content = msg.get("content", "")
                if content:
                    messages.append({"role": role, "content": content})

        messages.append({"role": "user", "content": user_query})

        # --- æ­¥é©Ÿ 4: å‘¼å« LLM ç”Ÿæˆå›ç­” ---
        print("  Asking LLM...")
        answer_text = ""
        suggestions = []

        try:
            # â˜…â˜…â˜… ä¿®å¾©é‡é» 2: é™ä½ Temperature æ¸›å°‘å¹»è¦º â˜…â˜…â˜…
            full_response = self.llm_client.call_gemini(messages=messages, temperature=0.1)
            answer_text = full_response

            # å˜—è©¦è§£æå»ºè­°å•é¡Œ
            suggestion_match = re.search(r'<suggestions>(.*?)</suggestions>', full_response, re.DOTALL)
            if suggestion_match:
                try:
                    json_str = suggestion_match.group(1).strip()
                    # æ¸…ç† Markdown èªæ³• (```json ... ```)
                    json_str = re.sub(r'```json\s*', '', json_str)
                    json_str = re.sub(r'```\s*', '', json_str)
                    
                    parsed = json.loads(json_str)
                    if isinstance(parsed, list) and len(parsed) > 0:
                        suggestions = parsed
                        # å¾å›ç­”ä¸­ç§»é™¤å»ºè­°å€å¡Š
                        answer_text = full_response.replace(suggestion_match.group(0), "").strip()
                except Exception as e:
                    print(f"âŒ JSON Parse Error: {e}")
            
            # â˜…â˜…â˜… ä¿®å¾©é‡é» 3: è² é¢å›ç­”æª¢æ¸¬èˆ‡å¼·åˆ¶ä¿åº• â˜…â˜…â˜…
            negative_keywords = ["æ‰¾ä¸åˆ°", "æœªæåŠ", "æ²’æœ‰æåˆ°", "ç„¡æ³•å›ç­”", "æŠ±æ­‰", "è³‡è¨Šä¸è¶³"]
            is_negative_answer = any(keyword in answer_text for keyword in negative_keywords)

            # æƒ…å¢ƒ A: AI èªªæ‰¾ä¸åˆ° -> å¼·åˆ¶æ›æˆæ‘˜è¦/é€šç”¨å¼•å°
            if is_negative_answer:
                print("  âš ï¸ Detected negative answer. Forcing fallback suggestions.")
                if results:
                    suggestions = [
                        "æ‘˜è¦é€™å¹¾ç¯‡å…¬å‘Š",
                        "åˆ—å‡ºç™¼å¸ƒæ—¥æœŸ",
                        "é€™å¹¾ç¯‡çš„é‡é»æ˜¯ä»€éº¼"
                    ]
                else:
                    suggestions = ["é‡æ–°æœå°‹", "ä½¿ç”¨ä¸åŒé—œéµå­—", "æœ€æ–°å…¬å‘Š"]
            
            # æƒ…å¢ƒ B: AI æ²’çµ¦å»ºè­° (è§£æå¤±æ•—æˆ–å¿˜äº†çµ¦) -> å¼·åˆ¶è£œä¸Š
            elif not suggestions:
                print("  âš ï¸ No suggestions found. Using fallback.")
                suggestions = ["æ‘˜è¦æœå°‹çµæœ", "åˆ—å‡ºé—œéµé‡é»", "é‚„æœ‰å…¶ä»–ç›¸é—œè³‡è¨Šå—ï¼Ÿ"]

            if not answer_text:
                answer_text = "æŠ±æ­‰ï¼Œç³»çµ±æš«æ™‚ç„¡æ³•ç”Ÿæˆå›æ‡‰ã€‚"

        except Exception as e:
            print(f"âŒ LLM Error: {e}")
            answer_text = "æŠ±æ­‰ï¼ŒAI æœå‹™é€£ç·šç™¼ç”ŸéŒ¯èª¤ã€‚"
            # å‡ºéŒ¯æ™‚ä¹Ÿè¦çµ¦æŒ‰éˆ•ï¼Œè®“ä½¿ç”¨è€…å¯ä»¥é‡è©¦
            suggestions = ["é‡æ–°æ•´ç†", "æª¢æŸ¥ç¶²è·¯", "é‡è©¦"]

        return {
            "answer": answer_text,
            "suggestions": suggestions, # é€™è£¡ä¿è­‰æ°¸é æœƒæœ‰ list
            "references": results if source_type == "search" else []
        }
    
    def summarize(self, user_query: str, search_results: List[Dict]) -> str:
        """
        é‡å°æœå°‹çµæœç”Ÿæˆæ‘˜è¦
        """
        print(f"ğŸ“ RAGService: Generating summary for '{user_query}'")
        
        if not search_results:
            return ""

        # 1. æº–å‚™ Context (åªå–å‰ 5 ç­†ï¼Œé¿å… Token å¤ªå¤š)
        context_text = ""
        for idx, doc in enumerate(search_results[:5], 1):
            title = doc.get('title', 'No Title')
            content = doc.get('content', '') or doc.get('cleaned_content', '')
            # æ‘˜è¦åªéœ€è¦éƒ¨åˆ†å…§å®¹å³å¯
            if len(content) > 500:
                content = content[:500] + "..."
            context_text += f"[ç¬¬ {idx} ç¯‡] æ¨™é¡Œ: {title}\nå…§å®¹: {content}\n\n"

        # 2. çµ„è£ Prompt
        from src.llm.rag_prompts import SUMMARY_SYSTEM_PROMPT
        prompt = SUMMARY_SYSTEM_PROMPT.format(context=context_text, query=user_query)
        
        messages = [{"role": "user", "content": prompt}]

        # 3. å‘¼å« LLM
        try:
            # ä½¿ç”¨è¼ƒä½çš„ temperature (0.3) è®“æ‘˜è¦æ›´ç©©å®š
            summary = self.llm_client.call_gemini(messages=messages, temperature=0.3)
            return summary
        except Exception as e:
            print(f"âŒ Summary Generation Error: {e}")
            return ""
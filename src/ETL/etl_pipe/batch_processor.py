"""
Batch Processor Module

Handles in-memory batch processing including:
- LLM metadata extraction
- Data merging
- Model switching on errors
"""

import json
import os
import sys
import time
import uuid
from typing import List, Dict, Any, Optional
from openai import APIStatusError

# Add project root to sys.path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../.."))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from src.llm.client import LLMClient
from src.llm.metadata_prompts import SYSTEM_PROMPT
from src.schema.schemas import (
    AnnouncementDoc,
    AnnouncementMetadata,
    BatchMetaExtraction,
    MetadataExtraction,
)
from src.ETL.etl_pipe.error_handler import ErrorHandler
from src.config import GEMINI_MODELS


class BatchProcessor:
    """è™•ç†è¨˜æ†¶é«”ä¸­æ‰¹æ¬¡è³‡æ–™çš„ ETL æµç¨‹ï¼Œæ”¯æ´æ¨¡å‹è‡ªå‹•åˆ‡æ›"""

    def __init__(
        self,
        llm_client: LLMClient,
        error_handler: ErrorHandler,
    ):
        self.llm_client = llm_client
        self.error_handler = error_handler

        # Model switching configuration
        self.available_models = GEMINI_MODELS.copy()
        self.current_model_index = 0

        # Set initial model from llm_client
        if hasattr(llm_client, "model") and llm_client.model in self.available_models:
            self.current_model_index = self.available_models.index(llm_client.model)

    def get_next_model(self) -> Optional[str]:
        """åˆ‡æ›åˆ°ä¸‹ä¸€å€‹å¯ç”¨çš„ model"""
        self.current_model_index += 1
        if self.current_model_index >= len(self.available_models):
            return None  # æ‰€æœ‰ model éƒ½è©¦éäº†

        next_model = self.available_models[self.current_model_index]
        print(f"ğŸ”„ åˆ‡æ›åˆ°ä¸‹ä¸€å€‹ model: {next_model}")
        return next_model

    def reset_model_index(self):
        """é‡ç½® model index åˆ°åˆå§‹å€¼ï¼ˆç”¨æ–¼æ–°çš„ batchï¼‰"""
        initial_model = (
            self.llm_client.model
            if hasattr(self.llm_client, "model")
            else self.available_models[0]
        )
        if initial_model in self.available_models:
            self.current_model_index = self.available_models.index(initial_model)
        else:
            self.current_model_index = 0

    def prepare_llm_input(
        self, raw_batch: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """æº–å‚™ LLM è¼¸å…¥è³‡æ–™ï¼ˆæ¨™æº–åŒ–æ¬„ä½ï¼‰"""
        llm_input = []
        for idx, item in enumerate(raw_batch):
            llm_item = {
                "id": item.get("id") or str(idx),
                "month": item.get("month"),
                "title": item.get("title"),
                "content": item.get("original_content") or item.get("content", ""),
            }
            llm_input.append(llm_item)
        return llm_input

    def extract_metadata(
        self, llm_input: List[Dict[str, Any]]
    ) -> Optional[BatchMetaExtraction]:
        """å‘¼å« LLM æå– metadataï¼Œæ”¯æ´æ¨¡å‹è‡ªå‹•åˆ‡æ›"""
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": json.dumps(llm_input, ensure_ascii=False)},
        ]

        # ä½¿ç”¨ç•¶å‰ model å˜—è©¦
        current_model = self.available_models[self.current_model_index]

        # å®‰å…¨æ©Ÿåˆ¶ï¼šæœ€å¤šå˜—è©¦æ‰€æœ‰å¯ç”¨ model ä¸€æ¬¡
        max_attempts = len(self.available_models)
        attempt_count = 0

        while attempt_count < max_attempts:
            attempt_count += 1

            try:
                print(
                    f"ğŸ“¡ ä½¿ç”¨ model: {current_model} (å˜—è©¦ {attempt_count}/{max_attempts})"
                )
                batch_result = self.llm_client.call_with_schema(
                    messages=messages,
                    response_model=BatchMetaExtraction,
                    model=current_model,
                )

                if batch_result:
                    return batch_result

                # å¦‚æœè¿”å› Noneï¼ˆé©—è­‰å¤±æ•—ï¼‰ï¼Œä¸åˆ‡æ› modelï¼Œç›´æ¥è¿”å› None
                return None

            except APIStatusError as e:
                # æª¢æŸ¥æ˜¯å¦ç‚º 429 æˆ– 500 éŒ¯èª¤
                if e.status_code in [429, 500]:
                    print(f"âš  é‡åˆ° {e.status_code} éŒ¯èª¤: {e.message}")

                    # å˜—è©¦åˆ‡æ›åˆ°ä¸‹ä¸€å€‹ model
                    next_model = self.get_next_model()
                    if next_model is None:
                        print(f"âœ— æ‰€æœ‰ model éƒ½å·²å˜—è©¦ï¼Œæ”¾æ£„è™•ç†")
                        return None

                    current_model = next_model
                    print(f"â³ ç­‰å¾… 3 ç§’å¾Œåˆ‡æ›...")
                    time.sleep(3)  # åˆ‡æ›å‰ç­‰å¾…ï¼Œé¿å…ç«‹å³å†æ¬¡è§¸ç™¼ rate limit
                    continue
                else:
                    # å…¶ä»–éŒ¯èª¤ç›´æ¥æ‹‹å‡º
                    print(f"âœ— API éŒ¯èª¤ ({e.status_code}): {e.message}")
                    raise

            except Exception as e:
                # å…¶ä»–ä¾‹å¤–ç›´æ¥æ‹‹å‡º
                print(f"âœ— æœªé æœŸçš„éŒ¯èª¤: {e}")
                raise

        # å®‰å…¨é€€å‡ºï¼šå¦‚æœè¶…éæœ€å¤§å˜—è©¦æ¬¡æ•¸
        print(f"âœ— å·²é”æœ€å¤§å˜—è©¦æ¬¡æ•¸ ({max_attempts})ï¼Œæ”¾æ£„è™•ç†")
        return None

    def merge_data(
        self, raw_items: List[Dict[str, Any]], metadata_items: List[MetadataExtraction]
    ) -> List[AnnouncementDoc]:
        """Merge raw items with extracted metadata (Pydantic objects)."""
        merged_docs = []

        # Create a lookup for metadata by ID (ID æ˜¯ id æˆ–ç´¢å¼•)
        metadata_by_id = {meta.id: meta for meta in metadata_items}

        for idx, raw in enumerate(raw_items):
            # æ‰¾å‡ºå°æ‡‰çš„ metadata (ä½¿ç”¨ id æˆ–ç´¢å¼•)
            lookup_id = raw.get("id") or str(idx)
            meta = metadata_by_id.get(lookup_id)

            if not meta:
                print(f"Warning: No metadata found for ID {lookup_id}, skipping.")
                continue

            # ä½¿ç”¨åŸå§‹çš„ id æˆ–ç”Ÿæˆæ–°çš„
            doc_id = raw.get("id") or str(uuid.uuid4())

            # Construct Metadata object from Pydantic MetadataExtraction
            try:
                metadata_obj = AnnouncementMetadata(
                    meta_date_effective=meta.meta_date_effective,
                    meta_products=meta.meta_products or [],
                    meta_category=meta.meta_category,
                    meta_audience=meta.meta_audience or [],
                    meta_impact_level=meta.meta_impact_level,
                    meta_action_deadline=meta.meta_action_deadline,
                    meta_summary=meta.meta_summary,
                    meta_change_type=meta.meta_change_type,
                    meta_date_announced=meta.meta_date_announced,
                )

                # Construct Document object
                doc = AnnouncementDoc(
                    id=doc_id,
                    month=raw.get("month", "Unknown"),
                    title=raw.get("title", ""),
                    link=raw.get("link"),
                    original_content=raw.get("original_content")
                    or raw.get("content", ""),
                    metadata=metadata_obj,
                )
                merged_docs.append(doc)
            except Exception as e:
                print(f"Error merging item {lookup_id}: {e}")

        return merged_docs

    def process_batch(
        self, raw_batch: List[Dict[str, Any]], batch_index: int
    ) -> Optional[List[AnnouncementDoc]]:
        """
        Process a single batch through the ETL pipeline.

        Args:
            raw_batch: List of raw announcement documents
            batch_index: Index of the batch for logging purposes

        Returns:
            List of processed AnnouncementDoc objects if successful, None if error occurred
        """
        print(f"Processing batch {batch_index} ({len(raw_batch)} items)...")
        llm_input = None
        ids = []

        try:
            # 1. Validate batch
            if not raw_batch:
                print("Empty batch, skipping.")
                return []  # Empty is not an error

            # Extract ids for error tracking
            ids = [item.get("id") or str(idx) for idx, item in enumerate(raw_batch)]

            # Reset model index for new batch
            self.reset_model_index()

            # 2. Check SYSTEM_PROMPT
            if not SYSTEM_PROMPT:
                error_msg = "SYSTEM_PROMPT is empty in src/llm/metadata_prompts.py"
                print(f"WARNING: {error_msg}")
                self.error_handler.log_error(
                    batch_file=f"batch_{batch_index}",
                    ids=ids,
                    error_type="ConfigError",
                    error_message=error_msg,
                )
                return None

            # 3. Prepare LLM Input
            llm_input = self.prepare_llm_input(raw_batch)

            # 4. Extract Metadata via LLM
            batch_result = self.extract_metadata(llm_input)

            if not batch_result:
                error_msg = (
                    "Failed to get validated response from LLM (all retries exhausted)"
                )
                print(f"âœ— {error_msg}")
                self.error_handler.log_error(
                    batch_file=f"batch_{batch_index}",
                    ids=ids,
                    error_type="LLMValidationError",
                    error_message=error_msg,
                    llm_input=llm_input,
                    llm_response=None,
                )
                return None

            # 5. Merge Data
            metadata_batch = batch_result.results
            docs = self.merge_data(raw_batch, metadata_batch)

            if len(docs) == 0:
                error_msg = (
                    f"Merge failed: No documents produced (expected {len(raw_batch)})"
                )
                print(f"âœ— {error_msg}")
                self.error_handler.log_error(
                    batch_file=f"batch_{batch_index}",
                    ids=ids,
                    error_type="MergeError",
                    error_message=error_msg,
                    llm_input=llm_input,
                    llm_response=json.dumps(
                        [m.model_dump() for m in metadata_batch], ensure_ascii=False
                    ),
                )
                return None

            # 6. Rate limit protection
            time.sleep(1)
            print(f"âœ“ Batch {batch_index} processed successfully ({len(docs)} docs)")
            return docs

        except Exception as e:
            error_msg = f"Unexpected error: {str(e)}"
            print(f"âœ— {error_msg}")
            self.error_handler.log_error(
                batch_file=f"batch_{batch_index}",
                ids=ids,
                error_type="UnexpectedError",
                error_message=error_msg,
                llm_input=llm_input,
                llm_response=None,
            )
            return None

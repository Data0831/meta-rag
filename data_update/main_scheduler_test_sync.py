import os
import importlib
import pkgutil
import schedule
import time
import json
import traceback
import logging
from datetime import datetime
from config.config import TimeConfig

# ==========================================
# ğŸ“¦ æ¨¡çµ„å¼•ç”¨
# ==========================================
try:
    from crawlers.base import BaseCrawler
    from core.diff_engine import process_diff_and_save
    from core.rag_sync import notify_rag_system
except ImportError as e:
    print(f"âŒ [ç³»çµ±éŒ¯èª¤] æ¨¡çµ„å¼•ç”¨å¤±æ•—: {e}")
    exit(1)

# ==========================================
# âš™ï¸ è¨­å®šå€
# ==========================================
UPDATE_DIR = os.path.join("data", "updates")
LOG_DIR = "logs"
LOG_FILE = os.path.join(LOG_DIR, "system_monitor.log")

# ==========================================
# ğŸ“ æ—¥èªŒç³»çµ±è¨­å®š (Logger Setup)
# ==========================================
def setup_logger():
    if not os.path.exists(LOG_DIR):
        os.makedirs(LOG_DIR)

    logger = logging.getLogger("Microsoft_QA_Scheduler")
    logger.setLevel(logging.INFO)
    
    if logger.handlers:
        return logger

    # 1. æª”æ¡ˆè¼¸å‡º
    file_handler = logging.FileHandler(LOG_FILE, encoding='utf-8')
    file_formatter = logging.Formatter('%(asctime)s - [%(levelname)s] - %(message)s')
    file_handler.setFormatter(file_formatter)
    logger.addHandler(file_handler)

    # 2. è¢å¹•è¼¸å‡º
    console_handler = logging.StreamHandler()
    console_formatter = logging.Formatter('%(message)s')
    console_handler.setFormatter(console_formatter)
    logger.addHandler(console_handler)

    return logger

logger = setup_logger()

# ==========================================
# ğŸ› ï¸ å·¥å…·å‡½å¼
# ==========================================

def load_crawlers():
    crawlers = []
    package_path = "crawlers"
    if not os.path.exists(package_path): return []
    
    for _, name, _ in pkgutil.iter_modules([package_path]):
        if name == "base": continue
        try:
            module = importlib.import_module(f"{package_path}.{name}")
            for attr_name in dir(module):
                attr = getattr(module, attr_name)
                if (isinstance(attr, type) and issubclass(attr, BaseCrawler) and attr is not BaseCrawler):
                    crawlers.append(attr())
        except Exception as e:
            logger.error(f"âŒ [ç³»çµ±] çˆ¬èŸ²æ¨¡çµ„ '{name}' è¼‰å…¥å¤±æ•—: {e}")
    return crawlers

def save_audit_files(source_name, diff_result):
    if not os.path.exists(UPDATE_DIR):
        os.makedirs(UPDATE_DIR)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    if diff_result.get("added"):
        filename = os.path.join(UPDATE_DIR, f"{source_name}_{timestamp}_to_add.json")
        try:
            with open(filename, "w", encoding="utf-8") as f:
                json.dump(diff_result["added"], f, ensure_ascii=False, indent=4)
        except Exception as e:
            logger.error(f"    âš ï¸ å¯«å…¥æ–°å¢ç´€éŒ„å¤±æ•—: {e}")

    if diff_result.get("deleted"):
        filename = os.path.join(UPDATE_DIR, f"{source_name}_{timestamp}_to_delete.json")
        try:
            ids = [chunk["id"] for chunk in diff_result["deleted"]]
            with open(filename, "w", encoding="utf-8") as f:
                json.dump(ids, f, ensure_ascii=False, indent=4)
        except Exception as e:
            logger.error(f"    âš ï¸ å¯«å…¥åˆªé™¤ç´€éŒ„å¤±æ•—: {e}")

# ==========================================
# ğŸš€ æ’ç¨‹æ ¸å¿ƒé‚è¼¯
# ==========================================

def job():
    logger.info(f"\nâ° [æœ¬åœ°åŒæ­¥æ¨¡å¼å•Ÿå‹•] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # æŒ‡å®šè¦æƒæçš„è³‡æ–™å¤¾
    DATA_SOURCE_DIR = "test_sync"  # å¦‚æœæ‚¨çš„æª”æ¡ˆåœ¨å…¶ä»–åœ°æ–¹ï¼Œè«‹ä¿®æ”¹é€™è£¡
    
    if not os.path.exists(DATA_SOURCE_DIR):
        logger.error(f"âŒ æ‰¾ä¸åˆ°è³‡æ–™å¤¾: {DATA_SOURCE_DIR}")
        return

    valid_diff_reports = []
    
    # éæ­·è³‡æ–™å¤¾ä¸­çš„æ‰€æœ‰ JSON æª”æ¡ˆ
    for filename in os.listdir(DATA_SOURCE_DIR):
        if filename.endswith(".json"):
            file_path = os.path.join(DATA_SOURCE_DIR, filename)
            logger.info(f"ğŸ“‚ æ­£åœ¨è™•ç†æœ¬åœ°æª”æ¡ˆ: {filename}")
            
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    
                    # æ¨¡æ“¬ diff_result æ ¼å¼
                    # æœ¬åœ°åŒæ­¥æ¨¡å¼æˆ‘å€‘å‡è¨­å…¨éƒ¨éƒ½æ˜¯ 'added'
                    diff_result = {
                        "source": filename,
                        "added": data if isinstance(data, list) else [data],
                        "deleted": [],
                        "status": "SUCCESS"
                    }
                    valid_diff_reports.append(diff_result)
                    
            except Exception as e:
                logger.error(f"âŒ è®€å–æª”æ¡ˆ {filename} å¤±æ•—: {e}")

    if not valid_diff_reports:
        logger.warning(f"âš ï¸ åœ¨ {DATA_SOURCE_DIR} ä¸­æœªæ‰¾åˆ°ä»»ä½•å¯è™•ç†çš„ JSON æª”æ¡ˆã€‚")
        return

    # --- ä»¥ä¸‹é€²å…¥ RAG Sync éšæ®µ ---

    # 3. å½™æ•´è¼¸å‡ºçµ¦ RAG
    if valid_diff_reports:
        logger.info(f"\nğŸ”„ å…±æœ‰ {len(valid_diff_reports)} å€‹ä¾†æºè®Šå‹•ï¼Œé–‹å§‹å‘¼å« RAG Sync...")
        try:
            notify_rag_system(valid_diff_reports)
            logger.info("ğŸ‰ RAG Sync å®Œæˆï¼ŒåŒæ­¥æª”æ¡ˆå·²ç”¢å‡ºã€‚")
        except Exception as e:
            logger.error(f"âŒ [RAG Sync] å¤±æ•—: {e}")
    else:
        logger.info("\nğŸ’¤ æœ¬æ¬¡æ’ç¨‹ç„¡æœ‰æ•ˆè®Šå‹•ï¼Œä¸ç”¢ç”Ÿ Sync æª”æ¡ˆã€‚")

    logger.info("âœ… æ’ç¨‹çµæŸã€‚\n" + "-"*40)

# ==========================================
# ğŸ ç¨‹å¼å…¥å£ (ä¿®æ”¹æ’ç¨‹æ™‚é–“)
# ==========================================

if __name__ == "__main__":
    logger.info("ğŸš€ ç³»çµ±å•Ÿå‹• (æ—¥èªŒç›£æ§ + ç†”æ–·ä¿è­· + å®šæ™‚ä»»å‹™)...")
    
    logger.info("âš¡ æ­£åœ¨åŸ·è¡Œã€åˆæ¬¡å•Ÿå‹•ã€‘æƒæä»»å‹™...")
    job() 
    logger.info("âœ… åˆæ¬¡æƒæå®Œæˆï¼Œè½‰å…¥æ’ç¨‹å¾…æ©Ÿæ¨¡å¼ã€‚")
    # è¨­å®šæ¯æ—¥å›ºå®šæ’ç¨‹
    schedule.every().day.at(TimeConfig.run_time[0]).do(job) # æ—©ä¸Š 6 é»
    
    logger.info(f"\nâ³ å·²è¨­å®šæ¯æ—¥æ’ç¨‹ï¼š{TimeConfig.run_time[0]} åŸ·è¡Œ...")
    
    while True:
        try:
            schedule.run_pending()
            time.sleep(TimeConfig.loop_time) # æ¯åˆ†é˜æª¢æŸ¥ä¸€æ¬¡æ™‚é–“
        except KeyboardInterrupt:
            logger.info("ğŸ›‘ ç³»çµ±å·²æ‰‹å‹•åœæ­¢ã€‚")
            break
        except Exception as e:
            logger.error(f"âŒ æ’ç¨‹è¿´åœˆç™¼ç”Ÿè‡´å‘½éŒ¯èª¤: {e}")
            time.sleep(60)
"""
æ¸¬è©¦è…³æœ¬ï¼šæ¯”è¼ƒåŸç‰ˆ split_text èˆ‡å„ªåŒ–ç‰ˆ split_text_optimized çš„æ•ˆèƒ½å’Œçµæœ
"""

import time
from shared_splitter import UnifiedTokenSplitter


def generate_test_text(length: int = 5000) -> str:
    """ç”Ÿæˆæ¸¬è©¦æ–‡æœ¬ï¼ˆä¸­è‹±æ··åˆï¼‰"""
    chinese_text = """
    äººå·¥æ™ºæ…§ï¼ˆArtificial Intelligence, AIï¼‰æ˜¯é›»è…¦ç§‘å­¸çš„ä¸€å€‹åˆ†æ”¯ï¼Œè‡´åŠ›æ–¼å‰µå»ºèƒ½å¤ åŸ·è¡Œé€šå¸¸éœ€è¦äººé¡æ™ºæ…§çš„ä»»å‹™çš„ç³»çµ±ã€‚
    é€™äº›ä»»å‹™åŒ…æ‹¬è¦–è¦ºæ„ŸçŸ¥ã€èªéŸ³è­˜åˆ¥ã€æ±ºç­–åˆ¶å®šå’Œèªè¨€ç¿»è­¯ç­‰ã€‚è¿‘å¹´ä¾†ï¼Œæ·±åº¦å­¸ç¿’æŠ€è¡“çš„çªç ´ä½¿å¾—AIåœ¨è¨±å¤šé ˜åŸŸå–å¾—äº†é¡¯è‘—é€²å±•ã€‚
    
    æ©Ÿå™¨å­¸ç¿’æ˜¯AIçš„æ ¸å¿ƒæŠ€è¡“ä¹‹ä¸€ã€‚å®ƒä½¿é›»è…¦ç³»çµ±èƒ½å¤ å¾æ•¸æ“šä¸­å­¸ç¿’ä¸¦æ”¹é€²ï¼Œè€Œç„¡éœ€æ˜ç¢ºç·¨ç¨‹ã€‚
    ç›£ç£å­¸ç¿’ã€éç›£ç£å­¸ç¿’å’Œå¼·åŒ–å­¸ç¿’æ˜¯ä¸‰ç¨®ä¸»è¦çš„æ©Ÿå™¨å­¸ç¿’æ–¹æ³•ã€‚æ¯ç¨®æ–¹æ³•éƒ½æœ‰å…¶ç¨ç‰¹çš„æ‡‰ç”¨å ´æ™¯å’Œå„ªå‹¢ã€‚
    
    è‡ªç„¶èªè¨€è™•ç†ï¼ˆNLPï¼‰æ˜¯AIçš„å¦ä¸€å€‹é‡è¦é ˜åŸŸï¼Œå°ˆæ³¨æ–¼ä½¿é›»è…¦èƒ½å¤ ç†è§£ã€è§£é‡‹å’Œç”Ÿæˆäººé¡èªè¨€ã€‚
    ç¾ä»£NLPç³»çµ±ä½¿ç”¨å¤§å‹èªè¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå¦‚GPTã€BERTç­‰ï¼Œé€™äº›æ¨¡å‹åœ¨å¤§é‡æ–‡æœ¬æ•¸æ“šä¸Šé€²è¡Œè¨“ç·´ã€‚
    
    é›»è…¦è¦–è¦ºæŠ€è¡“ä½¿æ©Ÿå™¨èƒ½å¤ å¾åœ–åƒæˆ–è¦–é »ä¸­ç²å–æœ‰æ„ç¾©çš„è³‡è¨Šã€‚å·ç©ç¥ç¶“ç¶²çµ¡ï¼ˆCNNï¼‰åœ¨åœ–åƒè­˜åˆ¥ä»»å‹™ä¸­è¡¨ç¾å‡ºè‰²ã€‚
    ç‰©é«”æª¢æ¸¬ã€åœ–åƒåˆ†å‰²å’Œäººè‡‰è­˜åˆ¥æ˜¯é›»è…¦è¦–è¦ºçš„å¸¸è¦‹æ‡‰ç”¨ã€‚é€™äº›æŠ€è¡“å·²å»£æ³›æ‡‰ç”¨æ–¼è‡ªå‹•é§•é§›ã€é†«ç™‚è¨ºæ–·ç­‰é ˜åŸŸã€‚
    """

    english_text = """
    Artificial Intelligence has revolutionized many industries. Machine learning algorithms can now 
    process vast amounts of data and identify patterns that humans might miss. Deep learning, 
    a subset of machine learning, uses neural networks with multiple layers to learn hierarchical 
    representations of data.
    
    Natural Language Processing enables computers to understand and generate human language. 
    Large Language Models have shown remarkable capabilities in tasks such as translation, 
    summarization, and question answering. These models are trained on billions of parameters 
    and can generate human-like text.
    
    Computer Vision allows machines to interpret visual information from the world. Convolutional 
    Neural Networks have achieved superhuman performance in image classification tasks. Object 
    detection and semantic segmentation are crucial for applications like autonomous vehicles.
    """

    # æ··åˆä¸­è‹±æ–‡ä¸¦é‡è¤‡åˆ°æŒ‡å®šé•·åº¦
    mixed_text = (chinese_text + "\n\n" + english_text) * (
        length // (len(chinese_text) + len(english_text)) + 1
    )
    return mixed_text[:length]


def test_splitter_performance():
    """æ¸¬è©¦ä¸¦æ¯”è¼ƒå…©ç¨®åˆ‡åˆ†æ–¹æ³•çš„æ•ˆèƒ½"""
    print("=" * 80)
    print("ğŸ§ª æ–‡æœ¬åˆ‡åˆ†å™¨æ•ˆèƒ½æ¸¬è©¦")
    print("=" * 80)

    # å‰µå»ºæ¸¬è©¦æ–‡æœ¬ï¼ˆä¸åŒé•·åº¦ï¼‰
    test_cases = [
        ("çŸ­æ–‡æœ¬", 1000),
        ("ä¸­ç­‰æ–‡æœ¬", 5000),
        ("é•·æ–‡æœ¬", 20000),
    ]

    for name, length in test_cases:
        print(f"\n{'='*80}")
        print(f"ğŸ“ æ¸¬è©¦æ¡ˆä¾‹: {name} ({length} å­—å…ƒ)")
        print(f"{'='*80}")

        test_text = generate_test_text(length)

        # æ¸¬è©¦åŸç‰ˆ
        splitter = UnifiedTokenSplitter(
            chunk_size=1500, overlap=300, tolerance=200, debug=False
        )

        start_time = time.time()
        chunks_original = splitter.split_text(test_text)
        time_original = time.time() - start_time

        print(f"\nğŸ“Š åŸç‰ˆ split_text:")
        print(f"   â±ï¸  åŸ·è¡Œæ™‚é–“: {time_original:.4f} ç§’")
        print(f"   ğŸ“¦ åˆ‡åˆ†æ®µæ•¸: {len(chunks_original)}")
        if chunks_original:
            tokens_per_chunk = [splitter.count_tokens(c) for c in chunks_original]
            print(
                f"   ğŸ“ Token æ•¸ç¯„åœ: {min(tokens_per_chunk)} - {max(tokens_per_chunk)}"
            )
            print(
                f"   ğŸ“ˆ å¹³å‡ Token æ•¸: {sum(tokens_per_chunk) / len(tokens_per_chunk):.1f}"
            )

        # æ¸¬è©¦å„ªåŒ–ç‰ˆ
        start_time = time.time()
        chunks_optimized = splitter.split_text_optimized(test_text)
        time_optimized = time.time() - start_time

        print(f"\nğŸ“Š å„ªåŒ–ç‰ˆ split_text_optimized:")
        print(f"   â±ï¸  åŸ·è¡Œæ™‚é–“: {time_optimized:.4f} ç§’")
        print(f"   ğŸ“¦ åˆ‡åˆ†æ®µæ•¸: {len(chunks_optimized)}")
        if chunks_optimized:
            tokens_per_chunk = [splitter.count_tokens(c) for c in chunks_optimized]
            print(
                f"   ğŸ“ Token æ•¸ç¯„åœ: {min(tokens_per_chunk)} - {max(tokens_per_chunk)}"
            )
            print(
                f"   ğŸ“ˆ å¹³å‡ Token æ•¸: {sum(tokens_per_chunk) / len(tokens_per_chunk):.1f}"
            )

        # æ•ˆèƒ½æå‡
        speedup = time_original / time_optimized if time_optimized > 0 else 0
        print(f"\nğŸš€ æ•ˆèƒ½æå‡: {speedup:.2f}x å€é€Ÿ")
        print(
            f"   â±ï¸  æ™‚é–“ç¯€çœ: {(time_original - time_optimized):.4f} ç§’ ({(1 - time_optimized/time_original)*100:.1f}%)"
        )

        # æª¢æŸ¥çµæœä¸€è‡´æ€§
        if len(chunks_original) == len(chunks_optimized):
            print(f"   âœ… åˆ‡åˆ†æ®µæ•¸ä¸€è‡´")
        else:
            print(
                f"   âš ï¸  åˆ‡åˆ†æ®µæ•¸ä¸åŒ (åŸç‰ˆ: {len(chunks_original)}, å„ªåŒ–ç‰ˆ: {len(chunks_optimized)})"
            )


def test_edge_cases():
    """æ¸¬è©¦é‚Šç•Œæƒ…æ³"""
    print("\n" + "=" * 80)
    print("ğŸ§ª é‚Šç•Œæƒ…æ³æ¸¬è©¦")
    print("=" * 80)

    splitter = UnifiedTokenSplitter(chunk_size=100, overlap=20, debug=True)

    test_cases = [
        ("ç©ºæ–‡æœ¬", ""),
        ("ç´”ç©ºç™½", "   \n\n  \t  "),
        ("è¶…çŸ­æ–‡æœ¬", "Hello, World!"),
        ("ç„¡æ¨™é»é•·æ–‡æœ¬", "a" * 1000),
        ("ç´”æ¨™é»", "ã€‚ï¼ï¼Ÿï¼Œï¼›ï¼šã€" * 50),
    ]

    for name, text in test_cases:
        print(f"\nğŸ“ æ¸¬è©¦: {name}")
        print(f"   æ–‡æœ¬é•·åº¦: {len(text)} å­—å…ƒ")

        try:
            chunks = splitter.split_text_optimized(text)
            print(f"   âœ… æˆåŠŸåˆ‡åˆ†ç‚º {len(chunks)} æ®µ")
        except Exception as e:
            print(f"   âŒ éŒ¯èª¤: {e}")


if __name__ == "__main__":
    # åŸ·è¡Œæ•ˆèƒ½æ¸¬è©¦
    test_splitter_performance()

    # åŸ·è¡Œé‚Šç•Œæ¸¬è©¦
    test_edge_cases()

    print("\n" + "=" * 80)
    print("âœ… æ¸¬è©¦å®Œæˆ")
    print("=" * 80)

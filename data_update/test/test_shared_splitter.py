"""
æ¸¬è©¦ UnifiedTokenSplitter åˆ‡åˆ†æ•ˆèƒ½èˆ‡æ­£ç¢ºæ€§
"""
import time
import sys
import os

# æ·»åŠ å°ˆæ¡ˆè·¯å¾‘
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.shared_splitter import UnifiedTokenSplitter


def generate_test_text(num_tokens: int = 50000) -> str:
    """ç”Ÿæˆæ¸¬è©¦ç”¨çš„é•·æ–‡æœ¬ (å„ªåŒ–ç‰ˆ)"""
    sample_paragraphs = [
        "é€™æ˜¯ä¸€æ®µæ¸¬è©¦æ–‡å­—ï¼Œç”¨æ–¼æ¸¬è©¦æ–‡æœ¬åˆ‡åˆ†å™¨çš„æ•ˆèƒ½ã€‚\n",
        "The quick brown fox jumps over the lazy dog.\n",
        "äººå·¥æ™ºæ…§æ­£åœ¨æ”¹è®Šæˆ‘å€‘çš„ç”Ÿæ´»æ–¹å¼ï¼Œå¾æ—¥å¸¸æ¶ˆè²»åˆ°ä¼æ¥­æ±ºç­–éƒ½æœ‰æ·±é å½±éŸ¿ã€‚\n",
        "Machine learning algorithms can process large amounts of data efficiently.\n",
        "åœ¨é€™å€‹æ•¸ä½æ™‚ä»£ï¼Œè³‡æ–™å·²æˆç‚ºæœ€é‡è¦çš„è³‡ç”¢ä¹‹ä¸€ã€‚\n",
        "Natural language processing enables computers to understand human language.\n",
        "å€å¡ŠéˆæŠ€è¡“æä¾›äº†ä¸€ç¨®å»ä¸­å¿ƒåŒ–çš„ä¿¡ä»»æ©Ÿåˆ¶ã€‚\n",
        "Cloud computing has revolutionized how businesses deploy applications.\n\n",
    ]
    
    # å…ˆè¨ˆç®—ä¸€å€‹ block çš„ token æ•¸ï¼Œæ¸›å°‘ count_tokens å‘¼å«æ¬¡æ•¸
    splitter = UnifiedTokenSplitter()
    one_block = "".join(sample_paragraphs)
    tokens_per_block = splitter.count_tokens(one_block)
    
    # è¨ˆç®—éœ€è¦å¤šå°‘å€‹ block
    blocks_needed = (num_tokens // tokens_per_block) + 1
    
    # ä¸€æ¬¡æ€§ç”Ÿæˆ
    text = one_block * blocks_needed
    
    return text


def test_performance(text: str, chunk_size: int = 500, overlap: int = 100):
    """æ¸¬è©¦åˆ‡åˆ†æ•ˆèƒ½"""
    splitter = UnifiedTokenSplitter(
        chunk_size=chunk_size,
        overlap=overlap,
        debug=False
    )
    
    total_tokens = splitter.count_tokens(text)
    print(f"ğŸ“Š æ¸¬è©¦æ–‡æœ¬: {len(text):,} å­—å…ƒ, {total_tokens:,} tokens")
    print(f"ğŸ”§ è¨­å®š: chunk_size={chunk_size}, overlap={overlap}")
    print("-" * 50)
    
    # è¨ˆæ™‚
    start_time = time.perf_counter()
    chunks = splitter.split_text(text)
    end_time = time.perf_counter()
    
    elapsed = end_time - start_time
    
    print(f"âœ… åˆ‡åˆ†å®Œæˆ!")
    print(f"   - åˆ‡åˆ†æ•¸é‡: {len(chunks)} æ®µ")
    print(f"   - è€—æ™‚: {elapsed:.3f} ç§’")
    print(f"   - é€Ÿåº¦: {total_tokens / elapsed:,.0f} tokens/ç§’")
    
    return chunks, elapsed


def test_correctness(chunks: list, splitter: UnifiedTokenSplitter, chunk_size: int, tolerance: int):
    """é©—è­‰åˆ‡åˆ†æ­£ç¢ºæ€§"""
    print("\nğŸ“ é©—è­‰åˆ‡åˆ†çµæœ...")
    
    all_valid = True
    for i, chunk in enumerate(chunks):
        token_count = splitter.count_tokens(chunk)
        
        # æª¢æŸ¥æ˜¯å¦è¶…éä¸Šé™ (chunk_size + tolerance)
        max_allowed = chunk_size + tolerance
        if token_count > max_allowed:
            print(f"   âŒ Chunk {i+1}: {token_count} tokens > {max_allowed} (è¶…éä¸Šé™!)")
            all_valid = False
        
        # æª¢æŸ¥é–‹é ­æ˜¯å¦ç‚ºæ›è¡Œå¾Œé–‹å§‹ï¼ˆé™¤äº†ç¬¬ä¸€æ®µï¼‰
        if i > 0:
            # å‰ä¸€æ®µçµå°¾æ‡‰è©²æ˜¯æ›è¡Œ
            prev_ends_with_newline = chunks[i-1].rstrip() != chunks[i-1]
            
    if all_valid:
        print("   âœ… æ‰€æœ‰ chunk éƒ½åœ¨å®¹è¨±ç¯„åœå…§")
    
    # çµ±è¨ˆ token åˆ†ä½ˆ
    token_counts = [splitter.count_tokens(c) for c in chunks]
    avg_tokens = sum(token_counts) / len(token_counts)
    min_tokens = min(token_counts)
    max_tokens = max(token_counts)
    
    print(f"\nğŸ“ˆ Token åˆ†ä½ˆçµ±è¨ˆ:")
    print(f"   - å¹³å‡: {avg_tokens:.1f} tokens")
    print(f"   - æœ€å°: {min_tokens} tokens")
    print(f"   - æœ€å¤§: {max_tokens} tokens")
    
    return all_valid


def test_newline_alignment(chunks: list):
    """æª¢æŸ¥æ›è¡Œå°é½Šæƒ…æ³"""
    print("\nğŸ” æª¢æŸ¥æ›è¡Œå°é½Š...")
    
    ends_with_newline = 0
    starts_with_newline = 0
    
    for i, chunk in enumerate(chunks):
        # æª¢æŸ¥åŸå§‹ chunkï¼ˆæœª stripï¼‰æ˜¯å¦ä»¥æ›è¡Œçµå°¾
        # æ³¨æ„ï¼šå› ç‚º split_text æœƒ stripï¼Œæ‰€ä»¥é€™è£¡æ”¹æˆæª¢æŸ¥æ˜¯å¦çœ‹èµ·ä¾†åƒå®Œæ•´è¡Œ
        if chunk.endswith('.') or chunk.endswith('ã€‚') or chunk.endswith('\n'):
            ends_with_newline += 1
    
    print(f"   - çµå°¾çœ‹èµ·ä¾†å®Œæ•´çš„æ®µè½: {ends_with_newline}/{len(chunks)}")


def main():
    print("=" * 60)
    print("ğŸ§ª UnifiedTokenSplitter æ•ˆèƒ½æ¸¬è©¦")
    print("=" * 60)
    
    # æ¸¬è©¦ä¸åŒè¦æ¨¡
    test_cases = [
        (10000, 500, 100),   # 1è¬ tokens
        (50000, 500, 100),   # 5è¬ tokens
        (50000, 500, 150),   # 5è¬ tokens, æ›´å¤§ overlap
    ]
    
    for num_tokens, chunk_size, overlap in test_cases:
        print(f"\n{'='*60}")
        print(f"ğŸ“¦ æ¸¬è©¦æ¡ˆä¾‹: {num_tokens:,} tokens, chunk={chunk_size}, overlap={overlap}")
        print("=" * 60)
        
        # ç”Ÿæˆæ¸¬è©¦æ–‡æœ¬ (è¨ˆæ™‚)
        print("â³ ç”Ÿæˆæ¸¬è©¦æ–‡æœ¬...")
        gen_start = time.perf_counter()
        text = generate_test_text(num_tokens)
        gen_elapsed = time.perf_counter() - gen_start
        print(f"   âœ… æ–‡æœ¬ç”Ÿæˆè€—æ™‚: {gen_elapsed:.3f} ç§’")
        
        # æ•ˆèƒ½æ¸¬è©¦
        splitter = UnifiedTokenSplitter(chunk_size=chunk_size, overlap=overlap)
        chunks, elapsed = test_performance(text, chunk_size, overlap)
        
        # æ­£ç¢ºæ€§é©—è­‰
        test_correctness(chunks, splitter, chunk_size, splitter.tolerance)
        
        # æ›è¡Œå°é½Šæª¢æŸ¥
        test_newline_alignment(chunks)
        
        # é¡¯ç¤ºå‰å¹¾å€‹ chunk çš„é–‹é ­å’Œçµå°¾
        print(f"\nğŸ“„ Chunk ç¯„ä¾‹ (å‰ 3 å€‹):")
        for i, chunk in enumerate(chunks[:3]):
            first_line = chunk.split('\n')[0][:50]
            last_line = chunk.split('\n')[-1][-50:]
            print(f"   Chunk {i+1}:")
            print(f"      é–‹é ­: {first_line}...")
            print(f"      çµå°¾: ...{last_line}")


if __name__ == "__main__":
    main()

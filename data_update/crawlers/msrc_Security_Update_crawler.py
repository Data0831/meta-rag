import time
import re
import os
import sys
import random
import pandas as pd
from urllib.parse import urljoin
from bs4 import BeautifulSoup
from markdownify import markdownify as md_converter
from datetime import datetime, timedelta

# ==========================================
# 1. è·¯å¾‘è¨­å®šï¼šç¢ºä¿èƒ½å¼•ç”¨ ../core/shared_splitter.py
# ==========================================
current_dir = os.path.dirname(os.path.abspath(__file__)) # å–å¾— crawlers è³‡æ–™å¤¾è·¯å¾‘
parent_dir = os.path.dirname(current_dir)                # å–å¾— Microsoft_QA è³‡æ–™å¤¾è·¯å¾‘
if parent_dir not in sys.path:
    sys.path.append(parent_dir)

# å˜—è©¦åŒ¯å…¥ UnifiedTokenSplitter
try:
    from core.shared_splitter import UnifiedTokenSplitter
except ImportError:
    print("âŒ [Error] ç„¡æ³•åŒ¯å…¥ UnifiedTokenSplitterï¼Œè«‹ç¢ºèª core/shared_splitter.py æ˜¯å¦å­˜åœ¨ã€‚")
    # é€™è£¡ä¸å¼·åˆ¶é€€å‡ºï¼Œé¿å… IDE æª¢æŸ¥å ±éŒ¯ï¼Œä½†åŸ·è¡Œæ™‚è‹¥ç„¡æ­¤æª”æœƒå¤±æ•—
    UnifiedTokenSplitter = None 

# ä¿ç•™ curl_cffi ç”¨æ–¼ç¹é WAF
from curl_cffi import requests

# å˜—è©¦å¼•ç”¨ BaseCrawlerï¼Œè‹¥ç„¡å‰‡ç¹¼æ‰¿ object
try:
    from .base import BaseCrawler
except ImportError:
    BaseCrawler = object

class MSRCGuideLocalCsvCrawler(BaseCrawler):
    """
    MSRC Update Guide çˆ¬èŸ² (æœ¬åœ° CSV ç‰ˆ)
    æ•´åˆè‡ªå‹•è·¯å¾‘åµæ¸¬èˆ‡ UnifiedTokenSplitter
    """

    def __init__(self, csv_file_path=None):
        """
        Args:
            csv_file_path (str, optional): CSV è·¯å¾‘ã€‚
            è‹¥æ’ç¨‹å™¨æœªæä¾›åƒæ•¸ (None)ï¼Œç¨‹å¼æœƒè‡ªå‹•é–å®šåŒç›®éŒ„ä¸‹çš„ 'MSRC_Request.csv'ã€‚
        """
        # ==========================================
        # 2. CSV è·¯å¾‘æ™ºæ…§åˆ¤æ–· (çµ•å°è·¯å¾‘è§£æ±ºæ–¹æ¡ˆ)
        # ==========================================
        # å–å¾—é€™æ”¯ç¨‹å¼ (msrc_spider.py) æ‰€åœ¨çš„çµ•å°è³‡æ–™å¤¾è·¯å¾‘
        # ä¾‹å¦‚: C:\Users\2512050\Desktop\Microsoft_QA\crawlers
        current_crawler_dir = os.path.dirname(os.path.abspath(__file__))
        
        # çµ„åˆå‡ºé è¨­ CSV çš„çµ•å°è·¯å¾‘
        default_absolute_path = os.path.join(current_crawler_dir, "MSRC_Request.csv")

        if csv_file_path is None:
            # æƒ…æ³ A: æ’ç¨‹å™¨æ²’å‚³åƒæ•¸ -> ä½¿ç”¨é è¨­çµ•å°è·¯å¾‘
            self.csv_file_path = default_absolute_path
            print(f"  ğŸ”§ æœªæŒ‡å®š CSV è·¯å¾‘ï¼Œè‡ªå‹•é–å®š: {self.csv_file_path}")
        else:
            # æƒ…æ³ B: æœ‰å‚³åƒæ•¸ -> æª¢æŸ¥æœ‰æ•ˆæ€§
            if os.path.exists(csv_file_path):
                self.csv_file_path = csv_file_path
            elif os.path.exists(default_absolute_path):
                # åƒæ•¸è·¯å¾‘ç„¡æ•ˆï¼Œä½†é è¨­è·¯å¾‘æœ‰æ•ˆ -> è‡ªå‹•ä¿®æ­£
                print(f"  âš ï¸ æŒ‡å®šè·¯å¾‘ '{csv_file_path}' ç„¡æ•ˆï¼Œè‡ªå‹•åˆ‡æ›è‡³é è¨­è·¯å¾‘: {default_absolute_path}")
                self.csv_file_path = default_absolute_path
            else:
                # éƒ½æ‰¾ä¸åˆ° -> ä¿ç•™åŸå€¼è®“å¾Œé¢å™´éŒ¯
                self.csv_file_path = csv_file_path

        self.base_url = "https://msrc.microsoft.com"
        
        # è¨­å®šä¸‹è¼‰ç›®éŒ„ (ä½¿ç”¨çµ•å°è·¯å¾‘)
        self.download_dir = os.path.join(current_crawler_dir, "downloads")
        if not os.path.exists(self.download_dir):
            os.makedirs(self.download_dir)

        self.impersonate_list = [
            "chrome110", "chrome119", "chrome120", 
            "edge99", "edge101", 
            "safari15_5"
        ]
        
        self.target_chunk_size = 1500
        self.overlap_size = 300
        self._init_tools()

    @property
    def source_name(self):
        return "msrc_kb_article"

    def _init_tools(self):
        """
        åˆå§‹åŒ– UnifiedTokenSplitter
        """
        if UnifiedTokenSplitter:
            print("  ğŸ”§ Initializing UnifiedTokenSplitter with tolerance=200...")
            self.text_splitter = UnifiedTokenSplitter(
                model_name="gpt-4o",
                chunk_size=self.target_chunk_size,
                overlap=self.overlap_size,
                tolerance=200  # ğŸ”¥ å®¹è¨±å€¼ï¼šç¸½é•· 1700 ä»¥å…§ä¸åˆ‡åˆ†
            )
        else:
            raise ImportError("UnifiedTokenSplitter not loaded.")

    def _create_single_shot_session(self):
        """å»ºç«‹ä¸€æ¬¡æ€§ Session"""
        browser_ver = random.choice(self.impersonate_list)
        session = requests.Session(impersonate=browser_ver)
        
        session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.9",
            "Referer": "https://msrc.microsoft.com/",
            "Connection": "close" 
        })
        return session

    def _fetch_and_parse_article(self, url):
        """æŠ“å–å–®ç¯‡æ–‡ç« """
        session = None
        try:
            session = self._create_single_shot_session()
            response = session.get(url, timeout=30)
            
            if response.status_code == 403: return "403_FORBIDDEN"
            if response.status_code == 404: return "404_NOT_FOUND"
            if response.status_code != 200:
                print(f"    âŒ HTTP Error {response.status_code}")
                return None

            html_content = response.text
            if "Access Denied" in html_content or "Request is blocked" in html_content:
                return "BLOCKED_CONTENT"

            soup = BeautifulSoup(html_content, 'html.parser')
            title_tag = soup.find('h1')
            title = title_tag.get_text(strip=True) if title_tag else "No Title"
            title = title.split(" - Microsoft")[0]

            # å˜—è©¦å®šä½ä¸»è¦å…§å®¹å€å¡Š
            content_div = soup.find('div', id='main') or \
                          soup.find('main') or \
                          soup.find('article') or \
                          soup.find('div', class_='support-content') or \
                          soup.find('div', class_='article-content') or \
                          soup.find('div', class_='ocpArticleContent') or \
                          soup.find('div', id='sup-article-content')
            
            if content_div:
                junk_selectors = ["script", "style", "nav", "footer", "button", ".no-print", ".sup-metablock", "#sup-article-feedback", ".wafer-cookie-banner"]
                for selector in junk_selectors:
                    for tag in content_div.select(selector): tag.extract()

                for a_tag in content_div.find_all('a', href=True):
                    if a_tag['href'].startswith('/'):
                        a_tag['href'] = urljoin(self.base_url, a_tag['href'])

                markdown_content = md_converter(str(content_div), heading_style="ATX")
                markdown_content = re.sub(r'\n{3,}', '\n\n', markdown_content)
                return {"title": title, "content": markdown_content}
            
            return None

        except Exception as e:
            error_str = str(e).lower()
            if "curl" in error_str or "connection" in error_str or "failed to connect" in error_str:
                return "CONNECTION_ERROR"
            print(f"    âŒ Generic Error: {e}")
            return None
        finally:
            if session: session.close()

    def _extract_date(self, text):
        if not text: return None
        match_chi = re.search(r'(\d{4})\s*å¹´\s*(\d{1,2})\s*æœˆ\s*(\d{1,2})\s*æ—¥', text)
        if match_chi:
            y, m, d = match_chi.groups()
            return f"{y}-{int(m):02d}-{int(d):02d}"
        match_eng = re.search(r'(January|February|March|April|May|June|July|August|September|October|November|December)\s+\d{1,2},\s+\d{4}', text, re.IGNORECASE)
        if match_eng:
            try:
                dt_obj = datetime.strptime(match_eng.group(0), "%B %d, %Y")
                return dt_obj.strftime("%Y-%m-%d")
            except ValueError: pass
        match_simple = re.search(r'(\d{4})[-/](\d{1,2})[-/](\d{1,2})', text)
        if match_simple:
            y, m, d = match_simple.groups()
            return f"{y}-{int(m):02d}-{int(d):02d}"
        return None

    def _create_chunks(self, kb_id, link, content_data, csv_date=None):
        """
        åˆ‡å¡Šé‚è¼¯
        :param csv_date: å¾ CSV å‚³å…¥çš„ Release Date (æ ¼å¼ YYYY-MM-DD)ï¼Œè‹¥æœ‰å‰‡å„ªå…ˆä½¿ç”¨
        """
        chunks = []
        raw_content = content_data['content']
        page_title = content_data['title']
        main_title = f"KB{kb_id}: {page_title}"
        
        # ğŸ”¥ [ä¿®æ”¹éƒ¨åˆ†] Year-Month é‚è¼¯
        # 1. å„ªå…ˆä½¿ç”¨ CSV æä¾›çš„æ—¥æœŸ
        if csv_date:
            # csv_date æ ¼å¼ç‚º '2025-01-15'ï¼Œåªå–å‰ 7 ç¢¼ => '2025-01'
            year_month_val = str(csv_date)[:7]
        else:
            # 2. å‚™æ´ï¼šè§£æç¶²é æ¨™é¡Œæˆ–å…§æ–‡
            extracted_date = self._extract_date(page_title)
            if not extracted_date:
                extracted_date = self._extract_date(raw_content[:500])
            
            # å¦‚æœæœ‰æŠ“åˆ°æ—¥æœŸ (YYYY-MM-DD)ï¼Œåªå–å‰ 7 ç¢¼ (YYYY-MM)
            if extracted_date:
                year_month_val = extracted_date[:7]
            else:
                year_month_val = "KB-Article"
        
        full_text = f"# {main_title}\n\n" + raw_content
        
        # ä½¿ç”¨ UnifiedTokenSplitter (å« tolerance)
        chunks_text = self.text_splitter.split_text(full_text)
        
        for chunk_content in chunks_text:
            chunk_obj = {
                "website": "MSRC Update Guide",
                "link": link,
                "heading_link": link,
                "year_month": year_month_val,  # é€™è£¡ç¾åœ¨æ˜¯ YYYY-MM
                "main_title": main_title,
                "title": f"Details for KB{kb_id}",
                "content": chunk_content,
                "kb_id": kb_id
            }
            chunks.append(chunk_obj)
        return chunks

    def run(self):
        print(f"ğŸš€ [MSRCGuideLocalCrawler] Starting Scraper (Local CSV Mode)...")
        all_final_dataset = []

        csv_path = self.csv_file_path
        if not os.path.exists(csv_path): 
            print(f"  ğŸ›‘ CSV file not found at: {csv_path}")
            # å†åšä¸€æ¬¡æœ€å¾Œç¢ºèªï¼Œå°å‡ºçµ•å°è·¯å¾‘å¹«åŠ©é™¤éŒ¯
            print(f"  (Checking Absolute Path: {os.path.abspath(csv_path)})")
            return []

        print(f"  ğŸ“– Processing CSV: {csv_path}")
        try:
            df = pd.read_csv(csv_path)
            
            # æª¢æŸ¥åŸºæœ¬æ¬„ä½
            required_cols = ['Article', 'Article (Link)']
            if not all(col in df.columns for col in required_cols):
                print(f"  ğŸ›‘ Missing columns. Expected {required_cols}. Found: {df.columns.tolist()}")
                return []
            
            # ğŸ”¥ [ä¿®æ”¹éƒ¨åˆ†] æ¬„ä½è™•ç†
            cols_to_keep = ['Article', 'Article (Link)']
            if 'Release Date' in df.columns:
                cols_to_keep.append('Release Date')
            
            df_filtered = df[cols_to_keep].copy()
            df_filtered['Article'] = df_filtered['Article'].astype(str).str.strip()
            df_filtered['Article (Link)'] = df_filtered['Article (Link)'].astype(str).str.strip()
            
            # è™•ç† Release Dateï¼šè½‰ç‚ºæ¨™æº– YYYY-MM-DD å­—ä¸²
            if 'Release Date' in df_filtered.columns:
                df_filtered['Release Date'] = pd.to_datetime(
                    df_filtered['Release Date'], errors='coerce'
                ).dt.strftime('%Y-%m-%d')
                # å°‡ NaT (ç©ºå€¼) è½‰ç‚º None
                df_filtered['Release Date'] = df_filtered['Release Date'].replace({float('nan'): None})

            # ç¯©é¸ç´”æ•¸å­— KB ID
            df_filtered = df_filtered[df_filtered['Article'].str.match(r'^\d+$')].drop_duplicates()
            
            if df_filtered.empty: 
                print("  âš ï¸ No valid articles found in CSV after filtering.")
                return []

        except Exception as e:
            print(f"  ğŸ›‘ CSV Error: {e}")
            return []

        print(f"  ğŸ•·ï¸ Starting crawl for {len(df_filtered)} articles...")
        
        total = len(df_filtered)
        count = 0

        for index, row in df_filtered.iterrows():
            count += 1
            kb_id = row['Article']
            link = row['Article (Link)']
            
            # ğŸ”¥ [ä¿®æ”¹éƒ¨åˆ†] å–å¾—æ—¥æœŸ
            release_date = row.get('Release Date')

            if not link.startswith("http"): link = urljoin(self.base_url, link)

            print(f"  Processing [{count}/{total}]: KB{kb_id} ...")

            max_retries = 5
            retry_count = 0
            success = False
            base_wait_time = 30

            while retry_count <= max_retries and not success:
                content_data = self._fetch_and_parse_article(link)

                if content_data in ["403_FORBIDDEN", "BLOCKED_CONTENT", "CONNECTION_ERROR"]:
                    retry_count += 1
                    wait_time = base_wait_time * retry_count
                    print(f"    â›” Blocked/Error ({content_data})! Sleeping for {wait_time}s...")
                    time.sleep(wait_time)
                    continue
                
                if content_data == "404_NOT_FOUND":
                    print("    âš ï¸ Page not found (404). Skipping.")
                    success = True 
                    break

                if content_data and isinstance(content_data, dict):
                    # ğŸ”¥ [ä¿®æ”¹éƒ¨åˆ†] å‚³å…¥ csv_date
                    chunks = self._create_chunks(kb_id, link, content_data, csv_date=release_date)
                    print(f"    âœ… Success | {len(chunks)} chunks")
                    all_final_dataset.extend(chunks)
                    success = True
                else:
                    print("    âš ï¸ Content Empty. Retrying...")
                    retry_count += 1
                    time.sleep(5)

            sleep_time = random.uniform(10, 20)
            print(f"    â˜• Resting ({sleep_time:.1f}s)...")
            time.sleep(sleep_time)

            if count % 3 == 0:
                print(f"    ğŸ›‘ [Deep Cooling] Batch of 3 done. Sleeping 120s...")
                time.sleep(120)

        print(f"âœ… Execution Complete. Total Chunks: {len(all_final_dataset)}")
        return all_final_dataset

if __name__ == "__main__":
    # æ¸¬è©¦å€ï¼šç•¶ç›´æ¥åŸ·è¡Œæ­¤è…³æœ¬æ™‚ (éé€é Scheduler)
    # ä¸å‚³åƒæ•¸ï¼Œæ¸¬è©¦è‡ªå‹•è·¯å¾‘æŠ“å–åŠŸèƒ½
    crawler = MSRCGuideLocalCsvCrawler()
    data = crawler.run()
    
    if data:
        df_result = pd.DataFrame(data)
        output_file = f"crawled_results_{int(time.time())}.csv"
        df_result.to_csv(output_file, index=False, encoding="utf-8-sig")
        print(f"çµæœå·²å„²å­˜è‡³: {output_file}")
import time
import re
import os
import sys
import json
import random
import pandas as pd
from urllib.parse import urljoin
from bs4 import BeautifulSoup
from markdownify import markdownify as md_converter
from datetime import datetime, timedelta

# ==========================================
# 0. è·¯å¾‘èˆ‡æ¨¡çµ„è¨­å®š
# ==========================================
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.append(parent_dir)

# å˜—è©¦åŒ¯å…¥ UnifiedTokenSplitter
try:
    from core.shared_splitter import UnifiedTokenSplitter
except ImportError:
    print("âŒ [Error] ç„¡æ³•åŒ¯å…¥ UnifiedTokenSplitterï¼Œè«‹ç¢ºèªè·¯å¾‘ã€‚")
    UnifiedTokenSplitter = None

from playwright.sync_api import sync_playwright
from curl_cffi import requests

try:
    from .base import BaseCrawler
except ImportError:
    BaseCrawler = object

class MSRCGuideCsvCrawler(BaseCrawler):
    """
    MSRC Update Guide çˆ¬èŸ² (é‡å°ä½¿ç”¨è€… HTML ä¿®æ­£ç‰ˆ)
    """

    def __init__(self):
        self.base_url = "https://msrc.microsoft.com"
        self.guide_url = "https://msrc.microsoft.com/update-guide/"
        
        self.download_dir = os.path.join(current_dir, "downloads")
        if not os.path.exists(self.download_dir):
            os.makedirs(self.download_dir)

        self.impersonate_list = [
            "chrome110", "chrome119", "chrome120", 
            "edge99", "edge101", 
            "safari15_5"
        ]
        
        self.target_chunk_size = 1500
        self.overlap_size = 300
        self._init_tools()

    @property
    def source_name(self):
        return "msrc_kb_article"

    def _init_tools(self):
        if UnifiedTokenSplitter:
            print("  ğŸ”§ Initializing UnifiedTokenSplitter...")
            self.text_splitter = UnifiedTokenSplitter(
                model_name="gpt-4o",
                chunk_size=self.target_chunk_size,
                overlap=self.overlap_size,
                tolerance=200
            )
        else:
            raise ImportError("UnifiedTokenSplitter is missing.")

    def _apply_stealth(self, page):
        """Playwright éš±èº«è¡“"""
        page.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined});")
        page.add_init_script("window.chrome = {runtime: {}};")
        page.add_init_script("Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]});")
        page.add_init_script("Object.defineProperty(navigator, 'languages', {get: () => ['en-US', 'en']});")

    def _create_single_shot_session(self):
        """å»ºç«‹ curl_cffi Session"""
        browser_ver = random.choice(self.impersonate_list)
        session = requests.Session(impersonate=browser_ver)
        
        session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.9",
            "Referer": "https://msrc.microsoft.com/",
            "Connection": "close" 
        })
        return session

    def _download_latest_csv_by_date(self, days=7):
        """
        ä¸‹è¼‰ CSVï¼šæ ¹æ“šä½¿ç”¨è€…æˆªåœ–ä¿®æ­£ (è§£æ±ºæ—¥æ›†æ“‹ä½ Ok æŒ‰éˆ•çš„å•é¡Œ)
        """
        print(f"  ğŸ“¥ [Phase 1] Navigating to MSRC to download CSV (Last {days} days)...")
        csv_path = None
        
        # è¨ˆç®—æ—¥æœŸ (æ ¼å¼èª¿æ•´ç‚º Jan 08, 2026 ä»¥ç¬¦åˆç¶²é )
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days)
        
        start_str = start_date.strftime("%b %d, %Y")
        end_str = end_date.strftime("%b %d, %Y")
        
        print(f"    ğŸ“… Target Range: {start_str} to {end_str}")

        with sync_playwright() as p:
            # å•Ÿå‹•ç€è¦½å™¨
            browser = p.chromium.launch(headless=False, args=["--disable-blink-features=AutomationControlled"])
            context = browser.new_context(
                accept_downloads=True, 
                viewport={'width': 1920, 'height': 1200}, 
                locale='en-US' 
            )
            page = context.new_page()
            self._apply_stealth(page)

            try:
                page.goto(self.guide_url, wait_until="networkidle", timeout=60000)
                time.sleep(5) 

                # 1. é–‹å•Ÿæ—¥æœŸé¸å–®
                try:
                    # ä½¿ç”¨ Icon éæ¿¾å®šä½æ—¥æœŸæŒ‰éˆ•
                    date_picker_btn = page.locator("[data-automationid='splitbuttonprimary']").filter(
                        has=page.locator("[data-icon-name='Calendar']")
                    ).first
                    
                    if date_picker_btn.is_visible():
                        date_picker_btn.click()
                        time.sleep(1.5)

                        # 2. å®šä½è¼¸å…¥æ¡† (æ ¹æ“šæ‚¨çš„ HTML è³‡è¨Šï¼ŒPlaceholder æ˜¯ä¸€æ¨£çš„)
                        date_inputs = page.get_by_placeholder("Select a date...")
                        
                        if date_inputs.count() >= 2:
                            # --- è¨­å®š Start Date (ç¬¬ 1 å€‹) ---
                            start_input = date_inputs.nth(0)
                            start_input.click()
                            start_input.fill(start_str)
                            print(f"    âŒ¨ï¸ Input Start Date: {start_str}")
                            
                            # --- è¨­å®š End Date (ç¬¬ 2 å€‹) ---
                            end_input = date_inputs.nth(1)
                            end_input.click()
                            end_input.fill(end_str)
                            print(f"    âŒ¨ï¸ Input End Date: {end_str}")
                            
                            # æŒ‰ä¸‹ Enter è§¸ç™¼æ—¥æœŸç¢ºèª
                            page.keyboard.press("Enter")
                            time.sleep(0.5)

                            # ==================================================
                            # ğŸ”¥ [é—œéµä¿®æ­£] é—œé–‰æ—¥æ›†å½ˆçª—ï¼Œé¿å…æ“‹ä½ Ok æŒ‰éˆ•
                            # ==================================================
                            print("    ğŸ›¡ï¸ Attempting to close calendar popup...")
                            # æ–¹æ³• A: æŒ‰ Esc
                            page.keyboard.press("Escape")
                            time.sleep(0.5)
                            
                            # æ–¹æ³• B: é»æ“Šä¸Šæ–¹æ¨™é¡Œæ–‡å­— (Select date range) å¼·åˆ¶å¤±ç„¦
                            # é€™æ˜¯æœ€ä¿éšªçš„åšæ³•ï¼Œé»æ“Šç©ºç™½è™•æˆ–æ¨™é¡Œ
                            try:
                                page.get_by_text("Select date range", exact=True).first.click(force=True)
                            except:
                                # å¦‚æœæ‰¾ä¸åˆ°æ¨™é¡Œï¼Œé»æ“Šè¼¸å…¥æ¡†æ—ç™½çš„ç©ºç™½è™• (body)
                                page.mouse.click(0, 0)
                            
                            time.sleep(1) # ç­‰å¾…æ—¥æ›†ç¸®å›å»

                        else:
                            print("    âš ï¸ Could not find date inputs.")

                        # 3. é»æ“Š OK (ç¾åœ¨æ‡‰è©²ä¸æœƒè¢«æ“‹ä½äº†)
                        ok_btn = page.get_by_role("button", name="Ok")
                        
                        if ok_btn.is_visible():
                            ok_btn.click()
                            print("    ğŸ‘‰ Clicked 'Ok'")
                        else:
                            # å¦‚æœé‚„æ˜¯é»ä¸åˆ°ï¼Œå˜—è©¦ç”¨ JS å¼·åˆ¶é»æ“Š
                            print("    âš ï¸ 'Ok' button not visible, trying force click...")
                            page.evaluate("document.querySelector('button[name=\"Ok\"]').click()")
                            
                        print("    â³ Waiting 5s for table refresh...")
                        time.sleep(5)
                    else:
                        print("    âš ï¸ Date Picker button not found.")
                except Exception as e:
                    print(f"    âš ï¸ Date setting failed: {e}")
                    # æˆªåœ–ä»¥ä¾¿å¾ŒçºŒé™¤éŒ¯
                    page.screenshot(path=os.path.join(self.download_dir, "error_date_click.png"))

                # 4. è§¸ç™¼åŒ¯å‡º (Download CSV)
                export_btn = page.locator("button[aria-label='Download']").first
                if not export_btn.is_visible(): export_btn = page.get_by_text("Download").first
                
                if export_btn.is_visible():
                    export_btn.click()
                    time.sleep(2)
                    
                    try:
                        # é¸æ“‡ CSV æ ¼å¼
                        csv_option = page.get_by_text("csv - Comma Separated Value", exact=False).first
                        if csv_option.is_visible(): csv_option.click()
                        
                        # å®šä½ Start æŒ‰éˆ• (åŒ¯å‡ºç¢ºèªè¦–çª—)
                        start_btn = page.get_by_role("button", name="Start").first
                        if not start_btn.is_visible(): start_btn = page.get_by_text("Start", exact=True).first
                        
                        if start_btn.is_visible():
                            # æª¢æŸ¥æ˜¯å¦æœ‰è³‡æ–™ (æŒ‰éˆ•æ˜¯å¦ Disabled)
                            if start_btn.is_disabled():
                                print("    ğŸ›‘ [Result] No data in range. 'Start' button is disabled.")
                                return None
                            
                            # ä¸‹è¼‰æª”æ¡ˆ
                            with page.expect_download(timeout=60000) as download_info:
                                start_btn.click()
                            
                            download = download_info.value
                            filename = f"msrc_kb_{int(time.time())}.csv"
                            save_path = os.path.join(self.download_dir, filename)
                            download.save_as(save_path)
                            print(f"    âœ… CSV Downloaded: {save_path}")
                            csv_path = save_path
                        else:
                            print("    âŒ 'Start' button not found.")
                    except Exception as e:
                        print(f"    âŒ Download interaction failed: {e}")
                else:
                    print("    âŒ Download (Export) button not found.")

            except Exception as e:
                print(f"  ğŸ›‘ Browser Automation failed: {e}")
            finally:
                browser.close()
        return csv_path

    def _fetch_and_parse_article(self, url):
        """æŠ“å–ä¸¦è§£æå–®ç¯‡æ–‡ç« """
        session = None
        try:
            session = self._create_single_shot_session()
            response = session.get(url, timeout=30)
            
            if response.status_code == 403: return "403_FORBIDDEN"
            if response.status_code == 404: return "404_NOT_FOUND"
            if response.status_code != 200: return None

            html_content = response.text
            if "Access Denied" in html_content: return "BLOCKED_CONTENT"

            soup = BeautifulSoup(html_content, 'html.parser')
            title_tag = soup.find('h1')
            title = title_tag.get_text(strip=True) if title_tag else "No Title"
            title = title.split(" - Microsoft")[0]

            content_div = soup.find('div', id='main') or soup.find('main') or soup.find('article')
            
            if content_div:
                for tag in content_div.select("script, style, nav, footer, button"): tag.extract()
                for a_tag in content_div.find_all('a', href=True):
                    if a_tag['href'].startswith('/'):
                        a_tag['href'] = urljoin(self.base_url, a_tag['href'])

                markdown_content = md_converter(str(content_div), heading_style="ATX")
                markdown_content = re.sub(r'\n{3,}', '\n\n', markdown_content)
                return {"title": title, "content": markdown_content}
            
            return None
        except Exception as e:
            return "CONNECTION_ERROR"
        finally:
            if session: session.close()

    def _extract_date(self, text):
        if not text: return None
        match = re.search(r'(\d{4})[-/](\d{1,2})[-/](\d{1,2})', text)
        if match:
            y, m, d = match.groups()
            return f"{y}-{int(m):02d}-{int(d):02d}"
        return None

    def _create_chunks(self, kb_id, link, content_data):
        chunks = []
        raw_content = content_data['content']
        page_title = content_data['title']
        main_title = f"KB{kb_id}: {page_title}"
        
        extracted_date = self._extract_date(page_title)
        if not extracted_date:
            extracted_date = self._extract_date(raw_content[:500])
        year_month_val = extracted_date if extracted_date else "KB-Article"
        
        full_text = f"# {main_title}\n\n" + raw_content
        
        chunks_text = self.text_splitter.split_text(full_text)
        
        for chunk_content in chunks_text:
            chunk_obj = {
                "source": self.source_name,
                "link": link,
                "heading_link": link,
                "year_month": year_month_val,
                "main_title": main_title,
                "title": f"Details for KB{kb_id}",
                "content": chunk_content,
                "kb_id": kb_id
            }
            chunks.append(chunk_obj)
        return chunks

    def run(self):
        print(f"ğŸš€ [MSRCGuideCrawler] Starting Scraper (Incremental Mode)...")
        
        # ==========================================
        # ğŸ”¥ [éœ€æ±‚ 3] è¼‰å…¥æ­·å²è³‡æ–™ (State Rehydration)
        # ==========================================
        history_file_path = os.path.join(parent_dir, "data", f"{self.source_name}.json")
        history_data = []
        
        if os.path.exists(history_file_path):
            print(f"  ğŸ“‚ Loading history from: {history_file_path}")
            try:
                with open(history_file_path, "r", encoding="utf-8") as f:
                    history_data = json.load(f)
                print(f"  âœ… Loaded {len(history_data)} existing records.")
            except Exception as e:
                print(f"  âš ï¸ Failed to load history: {e}")
        else:
            print("  ğŸ†• No history file found. Treating as fresh run.")

        # 1. ä¸‹è¼‰æœ€è¿‘ 7 å¤©çš„ CSV
        csv_path = self._download_latest_csv_by_date(days=7)
        
        # è‹¥ Start éµå¤±æ•ˆ (None)ï¼Œä»£è¡¨ç„¡è³‡æ–™ï¼Œç›´æ¥å›å‚³èˆŠè³‡æ–™
        if not csv_path:
            print("  ğŸ’¤ No new data found in the last 7 days. Returning history only.")
            return history_data

        # 2. è™•ç†æ–°è³‡æ–™
        new_chunks = []
        print(f"  ğŸ“– Processing CSV: {csv_path}")
        try:
            df = pd.read_csv(csv_path)
            if 'Article' in df.columns:
                df_filtered = df[df['Article'].astype(str).str.match(r'^\d+$')].drop_duplicates()
                print(f"  ğŸ•·ï¸ Found {len(df_filtered)} new articles to crawl...")

                count = 0
                for index, row in df_filtered.iterrows():
                    count += 1
                    kb_id = str(row['Article']).strip()
                    link = row.get('Article (Link)', '')
                    if not link.startswith("http"): link = urljoin(self.base_url, link)
                    
                    print(f"  Processing [{count}/{len(df_filtered)}]: KB{kb_id} ...")
                    
                    # åŸ·è¡Œé‡è©¦é‚è¼¯
                    success = False
                    retry_count = 0
                    while retry_count < 3 and not success:
                        content_data = self._fetch_and_parse_article(link)
                        if content_data and isinstance(content_data, dict):
                            chunks = self._create_chunks(kb_id, link, content_data)
                            new_chunks.extend(chunks)
                            print(f"    âœ… Parsed {len(chunks)} chunks")
                            success = True
                        else:
                            retry_count += 1
                            time.sleep(2)

                    time.sleep(random.uniform(2, 5)) # é¿å…è¢«æ“‹
            else:
                print("  âš ï¸ CSV format unexpected.")
        except Exception as e:
            print(f"  ğŸ›‘ CSV Processing Error: {e}")

        # 3. åˆä½µå›å‚³
        print(f"  ğŸ”„ Merging: {len(history_data)} (History) + {len(new_chunks)} (New)")
        # é€™è£¡ç°¡å–®ç›¸åŠ ï¼Œé‡è¤‡çš„é …ç›®æœƒåœ¨ Diff Engine ä¸­è¢«éæ¿¾æ‰ (å› ç‚º Hash ID ä¸€æ¨£)
        combined_dataset = history_data + new_chunks
        
        return combined_dataset

if __name__ == "__main__":
    crawler = MSRCGuideCsvCrawler()
    crawler.run()
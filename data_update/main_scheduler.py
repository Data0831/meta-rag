import os
import importlib
import pkgutil
import schedule
import time
import json
import traceback
import logging
from datetime import datetime

# ==========================================
# ğŸ“¦ æ¨¡çµ„å¼•ç”¨
# ==========================================
try:
    from crawlers.base import BaseCrawler
    from core.diff_engine import process_diff_and_save
    from core.rag_sync import notify_rag_system
except ImportError as e:
    print(f"âŒ [ç³»çµ±éŒ¯èª¤] æ¨¡çµ„å¼•ç”¨å¤±æ•—: {e}")
    exit(1)

# ==========================================
# âš™ï¸ è¨­å®šå€
# ==========================================
UPDATE_DIR = os.path.join("data", "updates")
LOG_DIR = "logs"
LOG_FILE = os.path.join(LOG_DIR, "system_monitor.log")

# ==========================================
# ğŸ“ æ—¥èªŒç³»çµ±è¨­å®š (Logger Setup)
# ==========================================
def setup_logger():
    if not os.path.exists(LOG_DIR):
        os.makedirs(LOG_DIR)

    logger = logging.getLogger("Microsoft_QA_Scheduler")
    logger.setLevel(logging.INFO)
    
    if logger.handlers:
        return logger

    # 1. æª”æ¡ˆè¼¸å‡º
    file_handler = logging.FileHandler(LOG_FILE, encoding='utf-8')
    file_formatter = logging.Formatter('%(asctime)s - [%(levelname)s] - %(message)s')
    file_handler.setFormatter(file_formatter)
    logger.addHandler(file_handler)

    # 2. è¢å¹•è¼¸å‡º
    console_handler = logging.StreamHandler()
    console_formatter = logging.Formatter('%(message)s')
    console_handler.setFormatter(console_formatter)
    logger.addHandler(console_handler)

    return logger

logger = setup_logger()

# ==========================================
# ğŸ› ï¸ å·¥å…·å‡½å¼
# ==========================================

def load_crawlers():
    crawlers = []
    package_path = "crawlers"
    if not os.path.exists(package_path): return []
    
    for _, name, _ in pkgutil.iter_modules([package_path]):
        if name == "base": continue
        try:
            module = importlib.import_module(f"{package_path}.{name}")
            for attr_name in dir(module):
                attr = getattr(module, attr_name)
                if (isinstance(attr, type) and issubclass(attr, BaseCrawler) and attr is not BaseCrawler):
                    crawlers.append(attr())
        except Exception as e:
            logger.error(f"âŒ [ç³»çµ±] çˆ¬èŸ²æ¨¡çµ„ '{name}' è¼‰å…¥å¤±æ•—: {e}")
    return crawlers

def save_audit_files(source_name, diff_result):
    if not os.path.exists(UPDATE_DIR):
        os.makedirs(UPDATE_DIR)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    if diff_result.get("added"):
        filename = os.path.join(UPDATE_DIR, f"{source_name}_{timestamp}_to_add.json")
        try:
            with open(filename, "w", encoding="utf-8") as f:
                json.dump(diff_result["added"], f, ensure_ascii=False, indent=4)
        except Exception as e:
            logger.error(f"    âš ï¸ å¯«å…¥æ–°å¢ç´€éŒ„å¤±æ•—: {e}")

    if diff_result.get("deleted"):
        filename = os.path.join(UPDATE_DIR, f"{source_name}_{timestamp}_to_delete.json")
        try:
            ids = [chunk["id"] for chunk in diff_result["deleted"]]
            with open(filename, "w", encoding="utf-8") as f:
                json.dump(ids, f, ensure_ascii=False, indent=4)
        except Exception as e:
            logger.error(f"    âš ï¸ å¯«å…¥åˆªé™¤ç´€éŒ„å¤±æ•—: {e}")

# ==========================================
# ğŸš€ æ’ç¨‹æ ¸å¿ƒé‚è¼¯
# ==========================================

def job():
    logger.info(f"\nâ° [æ’ç¨‹å•Ÿå‹•] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    active_crawlers = load_crawlers()
    
    if not active_crawlers:
        logger.warning("âš ï¸ æœªåµæ¸¬åˆ°ä»»ä½•æœ‰æ•ˆçš„çˆ¬èŸ²æ¨¡çµ„ã€‚")
        return

    valid_diff_reports = []
    summary_logs = []

    for crawler in active_crawlers:
        source_name = getattr(crawler, 'source_name', 'Unknown')
        print(f"=== ä»»å‹™å•Ÿå‹•: {source_name} ===") 
        
        try:
            start_time = time.time()
            final_chunks = crawler.run()
            duration = time.time() - start_time
            
            # ğŸ›¡ï¸ [é˜²å‘† 1] ç©ºè³‡æ–™ä¿è­·
            if not final_chunks:
                msg = f"âš ï¸ [ç•°å¸¸] {source_name}: çˆ¬èŸ²å›å‚³ 0 ç­†è³‡æ–™ (è€—æ™‚ {duration:.1f}s)ã€‚å·²è·³éæ¯”å°ã€‚"
                logger.warning(msg)
                summary_logs.append(msg)
                continue

            # 2. åŸ·è¡Œ Diff Engine
            diff_result = process_diff_and_save(source_name, final_chunks)
            
            if diff_result:
                status = diff_result.get("status")
                add_count = len(diff_result.get("added", []))
                del_count = len(diff_result.get("deleted", []))

                # ğŸ›¡ï¸ [é˜²å‘† 2] ç†”æ–·æª¢æŸ¥
                if status == "CIRCUIT_BREAKER_TRIGGERED":
                    msg = f"ğŸš« [ç†”æ–·] {source_name}: è©¦åœ–åˆªé™¤ {del_count} ç­† (è¶…é 1/3)ã€‚æ›´æ–°å·²æ””æˆªã€‚"
                    logger.warning(msg)
                    summary_logs.append(msg)
                    continue
                
                elif status == "SUCCESS":
                    save_audit_files(source_name, diff_result)
                    valid_diff_reports.append(diff_result)
                    msg = f"âœ… [æˆåŠŸ] {source_name}: æ–°å¢ {add_count}, åˆªé™¤ {del_count} (è€—æ™‚ {duration:.1f}s)"
                    logger.info(msg)
                    summary_logs.append(msg)
            else:
                msg = f"ğŸ’¤ [ç„¡è®Šå‹•] {source_name} (è€—æ™‚ {duration:.1f}s)"
                logger.info(f"    {msg}")
                summary_logs.append(msg)
                
        except Exception as e:
            error_msg = f"âŒ [éŒ¯èª¤] {source_name}: {str(e)}"
            logger.error(error_msg)
            logger.error(traceback.format_exc())
            summary_logs.append(error_msg)

    # 3. å½™æ•´è¼¸å‡ºçµ¦ RAG
    if valid_diff_reports:
        logger.info(f"\nğŸ”„ å…±æœ‰ {len(valid_diff_reports)} å€‹ä¾†æºè®Šå‹•ï¼Œé–‹å§‹å‘¼å« RAG Sync...")
        try:
            notify_rag_system(valid_diff_reports)
            logger.info("ğŸ‰ RAG Sync å®Œæˆï¼ŒåŒæ­¥æª”æ¡ˆå·²ç”¢å‡ºã€‚")
        except Exception as e:
            logger.error(f"âŒ [RAG Sync] å¤±æ•—: {e}")
    else:
        logger.info("\nğŸ’¤ æœ¬æ¬¡æ’ç¨‹ç„¡æœ‰æ•ˆè®Šå‹•ï¼Œä¸ç”¢ç”Ÿ Sync æª”æ¡ˆã€‚")

    logger.info("âœ… æ’ç¨‹çµæŸã€‚\n" + "-"*40)

# ==========================================
# ğŸ ç¨‹å¼å…¥å£ (ä¿®æ”¹æ’ç¨‹æ™‚é–“)
# ==========================================

if __name__ == "__main__":
    logger.info("ğŸš€ ç³»çµ±å•Ÿå‹• (æ—¥èªŒç›£æ§ + ç†”æ–·ä¿è­· + å®šæ™‚ä»»å‹™)...")
    
    logger.info("âš¡ æ­£åœ¨åŸ·è¡Œã€åˆæ¬¡å•Ÿå‹•ã€‘æƒæä»»å‹™...")
    job() 
    logger.info("âœ… åˆæ¬¡æƒæå®Œæˆï¼Œè½‰å…¥æ’ç¨‹å¾…æ©Ÿæ¨¡å¼ã€‚")
    # è¨­å®šæ¯æ—¥å›ºå®šæ’ç¨‹
    schedule.every().day.at("06:00").do(job) # æ—©ä¸Š 6 é»
    
    logger.info(f"\nâ³ å·²è¨­å®šæ¯æ—¥æ’ç¨‹ï¼š06:00 èˆ‡ 18:00 åŸ·è¡Œ...")
    
    while True:
        try:
            schedule.run_pending()
            time.sleep(60) # æ¯åˆ†é˜æª¢æŸ¥ä¸€æ¬¡æ™‚é–“
        except KeyboardInterrupt:
            logger.info("ğŸ›‘ ç³»çµ±å·²æ‰‹å‹•åœæ­¢ã€‚")
            break
        except Exception as e:
            logger.error(f"âŒ æ’ç¨‹è¿´åœˆç™¼ç”Ÿè‡´å‘½éŒ¯èª¤: {e}")
            time.sleep(60)
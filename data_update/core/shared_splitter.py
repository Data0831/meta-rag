import tiktoken
from typing import List, Optional
from config.config import TokenConfig


class UnifiedTokenSplitter:
    def __init__(
        self,
        model_name: str = TokenConfig.MODEL_NAME,
        chunk_size: int = TokenConfig.CHUNK_SIZE,
        overlap: int = TokenConfig.OVERLAP,
        tolerance: int = TokenConfig.TOLERANCE,
        debug: bool = False,
    ):
        """
        é«˜æ•ˆèƒ½ç²¾æº–åˆ‡åˆ†å™¨ï¼šé å…ˆ tokenize + æ›è¡Œå„ªå…ˆåˆ‡åˆ†ã€‚

        åƒæ•¸èªªæ˜ï¼š
        :param model_name: ä½¿ç”¨çš„æ¨¡å‹åç¨±ï¼ˆç”¨æ–¼é¸æ“‡å°æ‡‰çš„ tokenizerï¼‰
        :param chunk_size: [ç›®æ¨™å¤§å°] æ¯ä¸€æ®µæœŸæœ›çš„ Token æ•¸é‡é™åˆ¶ã€‚
        :param overlap: [é‡è¤‡å€é–“] ç›¸é„°å…©æ®µä¹‹é–“ã€Œé‡ç–Šã€çš„éƒ¨åˆ†ã€‚
                        ğŸ’¡ ä¸‹ä¸€æ®µæœƒå¾å‰ä¸€æ®µçµå°¾ ~overlap ç¯„åœå…§çš„æ›è¡Œè™•é–‹å§‹ã€‚
        :param tolerance: [å¯¬å®¹ç·©è¡] å¦‚æœæœ€å¾Œä¸€æ®µåªå‰©ä¸‹ä¸€é»é»ï¼ˆToken æ•¸ < chunk_size + toleranceï¼‰ï¼Œ
                          å°±ç›´æ¥åˆä½µï¼Œä¸å†åˆ‡åˆ†ï¼Œé¿å…å‡ºç¾æ¥µçŸ­ç‰‡æ®µã€‚
        :param debug: æ˜¯å¦å•Ÿç”¨èª¿è©¦æ¨¡å¼ï¼ˆè¼¸å‡ºè©³ç´°æ—¥èªŒï¼‰
        """
        self.chunk_size = chunk_size
        self.overlap = overlap
        self.tolerance = tolerance
        self.debug = debug

        # æ”¹é€²ç•°å¸¸è™•ç†ï¼šåªæ•ç²ç‰¹å®šç•°å¸¸
        try:
            self.enc = tiktoken.encoding_for_model(model_name)
        except (KeyError, ValueError) as e:
            if self.debug:
                print(
                    f"âš ï¸ ç„¡æ³•è¼‰å…¥æ¨¡å‹ {model_name} çš„ tokenizerï¼Œä½¿ç”¨é è¨­ cl100k_base: {e}"
                )
            self.enc = tiktoken.get_encoding("cl100k_base")

        # æ¨™é»å„ªå…ˆç´šï¼šæ›è¡Œ > å¥æœ«æ¨™é» > åˆ†è™Ÿ/å†’è™Ÿ > é€—è™Ÿ/é “è™Ÿ
        self.separators = [
            "\n\n",
            "\n",
            "\r\n",  # Windows æ›è¡Œ
            "ã€‚",
            "ï¼",
            "ï¼Ÿ",
            "!",
            "?",
            "ï¼›",
            ";",
            "ï¼š",  # ä¸­æ–‡å†’è™Ÿ
            ":",  # è‹±æ–‡å†’è™Ÿ
            "ï¼Œ",
            ",",
            "ã€",  # ä¸­æ–‡é “è™Ÿ
        ]

    def count_tokens(self, text: str) -> int:
        if not text:
            return 0
        return len(self.enc.encode(text))

    def split_text(self, text: str) -> List[str]:
        """
        é«˜æ•ˆèƒ½æ–‡æœ¬åˆ‡åˆ†ï¼šé å…ˆ tokenize + æ›è¡Œå„ªå…ˆåˆ‡åˆ†ã€‚
        
        âš¡ æ•ˆèƒ½å„ªåŒ–ï¼š
        - é å…ˆ tokenize ä¸€æ¬¡ â†’ O(n) è€Œé O(n Ã— chunks)
        
        ğŸ’¡ åˆ‡åˆ†ç­–ç•¥ï¼š
        - chunk çµå°¾ç›¡é‡åˆ‡åœ¨æ›è¡Œç¬¦
        - ä¸‹ä¸€å€‹ chunk çš„èµ·é»åœ¨ã€Œå‰ä¸€æ®µçµå°¾ ~ overlap ç¯„åœå…§ã€çš„æ›è¡Œè™•
        - æ‰¾ä¸åˆ°æ›è¡Œæ™‚æ‰ä½¿ç”¨æ¨™é»ï¼Œå†æ‰¾ä¸åˆ°æ‰ç¡¬åˆ‡

        :param text: è¦åˆ‡åˆ†çš„æ–‡æœ¬
        :return: åˆ‡åˆ†å¾Œçš„æ–‡æœ¬ç‰‡æ®µåˆ—è¡¨
        """
        if not text or not text.strip():
            return []

        # ä¸€æ¬¡æ€§ tokenize æ•´å€‹æ–‡æœ¬
        all_tokens = self.enc.encode(text)
        total_tokens = len(all_tokens)

        if self.debug:
            print(f"ğŸ“Š æ–‡æœ¬ç¸½é•·åº¦: {len(text)} å­—å…ƒ, {total_tokens} tokens")

        # ç·©è¡æª¢æŸ¥ï¼šå¦‚æœç¸½é•·åº¦åœ¨ (ç›®æ¨™ + å¯¬å®¹å€¼) å…§ï¼Œç›´æ¥å›å‚³
        if total_tokens <= (self.chunk_size + self.tolerance):
            if self.debug:
                print(f"âœ… æ–‡æœ¬é•·åº¦åœ¨å®¹è¨±ç¯„åœå…§ï¼Œä¸éœ€åˆ‡åˆ†")
            return [text.strip()]

        chunks = []
        token_start = 0

        while token_start < total_tokens:
            remaining = total_tokens - token_start

            # å¦‚æœå‰©é¤˜ tokens ä¸å¤šï¼Œç›´æ¥å…¨åŒ…
            if remaining <= (self.chunk_size + self.tolerance):
                chunk_tokens = all_tokens[token_start:]
                chunk_text = self.enc.decode(chunk_tokens).strip()
                if chunk_text:
                    chunks.append(chunk_text)
                    if self.debug:
                        print(f"âœ… æœ€å¾Œä¸€æ®µ (tokens: {remaining}): {chunk_text[:50]}...")
                break

            # å–å¾—ç¡¬æ€§ä¸Šé™ç¯„åœçš„ tokens (chunk_size)
            token_end = min(token_start + self.chunk_size, total_tokens)
            chunk_tokens = all_tokens[token_start:token_end]
            chunk_text = self.enc.decode(chunk_tokens)

            # === æ™ºæ…§å°‹æ‰¾åˆ‡å‰²é»ï¼ˆå„ªå…ˆæ›è¡Œï¼‰ ===
            # åœ¨ chunk å°¾ç«¯æœå°‹æœ€ä½³åˆ‡åˆ†ä½ç½®
            search_char_range = min(len(chunk_text) // 4, 300)  # æœå°‹ç¯„åœï¼šæœ€å¾Œ 1/4 æˆ– 300 å­—å…ƒ
            search_start_char = max(0, len(chunk_text) - search_char_range)
            snippet = chunk_text[search_start_char:]

            best_offset = len(chunk_text)  # é è¨­ä½¿ç”¨å®Œæ•´ chunk
            for sep in self.separators:
                found_idx = snippet.rfind(sep)
                if found_idx != -1:
                    best_offset = search_start_char + found_idx + len(sep)
                    break

            # å–å¾—æœ€çµ‚çš„ chunk æ–‡æœ¬
            final_chunk_text = chunk_text[:best_offset].strip()
            if not final_chunk_text:
                # å¦‚æœ strip å¾Œç‚ºç©ºï¼Œä½¿ç”¨å®Œæ•´ chunk
                final_chunk_text = chunk_text.strip()
                best_offset = len(chunk_text)

            if final_chunk_text:
                chunks.append(final_chunk_text)
                if self.debug:
                    chunk_token_count = len(self.enc.encode(final_chunk_text))
                    print(f"ğŸ“ Chunk {len(chunks)} (tokens: {chunk_token_count}): {final_chunk_text[:50]}...")

            # === è¨ˆç®—ä¸‹ä¸€æ®µçš„èµ·å§‹ token ä½ç½®ï¼ˆæ›è¡Œå„ªå…ˆ overlapï¼‰===
            # ç­–ç•¥ï¼šåœ¨ chunk çµå°¾ ~ overlap ç¯„åœå…§æ‰¾æ›è¡Œï¼Œå¾é‚£è£¡é–‹å§‹ä¸‹ä¸€æ®µ
            
            # è¨ˆç®—é€™å€‹ chunk å¯¦éš›ä½¿ç”¨çš„ token æ•¸
            final_chunk_tokens = self.enc.encode(final_chunk_text)
            actual_chunk_token_count = len(final_chunk_tokens)
            
            # overlap å€åŸŸ
            overlap_token_count = min(self.overlap, actual_chunk_token_count)
            
            if overlap_token_count > 0:
                # å–å¾— overlap å€åŸŸçš„æ–‡å­—
                overlap_tokens = final_chunk_tokens[-overlap_token_count:]
                overlap_text = self.enc.decode(overlap_tokens)
                
                # åœ¨ overlap å€åŸŸå…§æ‰¾æ›è¡Œï¼ˆå¾å‰å¾€å¾Œæ‰¾ï¼Œé€™æ¨£ overlap æœƒæ›´å¤§ï¼‰
                newline_pos = overlap_text.find('\n')
                if newline_pos != -1:
                    # æ‰¾åˆ°æ›è¡Œï¼Œå¾æ›è¡Œå¾Œé–‹å§‹
                    # è¨ˆç®—æ›è¡Œå‰çš„ tokens æ•¸
                    text_before_newline = overlap_text[:newline_pos + 1]
                    tokens_before_newline = len(self.enc.encode(text_before_newline))
                    # ä¸‹ä¸€æ®µå¾ overlap é–‹å§‹ä½ç½® + æ›è¡Œå‰çš„ tokens é–‹å§‹
                    skip_tokens = actual_chunk_token_count - overlap_token_count + tokens_before_newline
                    token_start = token_start + skip_tokens
                else:
                    # æ²’æ‰¾åˆ°æ›è¡Œï¼Œä½¿ç”¨æ¨™æº– overlap
                    token_start = token_start + actual_chunk_token_count - overlap_token_count
            else:
                token_start = token_start + actual_chunk_token_count

            # å®‰å…¨æª¢æŸ¥ï¼šç¢ºä¿æœ‰å‰é€²
            if token_start >= total_tokens:
                break
            # é˜²æ­¢ç„¡é™å¾ªç’°
            if actual_chunk_token_count == 0:
                token_start += 1

        if self.debug:
            print(f"âœ… åˆ‡åˆ†å®Œæˆï¼Œå…± {len(chunks)} æ®µ")

        return chunks
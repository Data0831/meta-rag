import tiktoken
from typing import List, Optional


class UnifiedTokenSplitter:
    def __init__(
        self,
        model_name: str = "gpt-4o-mini",
        chunk_size: int = 1500,
        overlap: int = 300,
        tolerance: int = 200,
        debug: bool = False,
        table_aware: bool = True,
    ):
        """
        æ··åˆç‰ˆç²¾æº–åˆ‡åˆ†å™¨ï¼šçµåˆ Token è¨ˆæ•¸èˆ‡æ™ºæ…§æ¨™é»è­˜åˆ¥ã€‚

        åƒæ•¸èªªæ˜ï¼š
        :param model_name: ä½¿ç”¨çš„æ¨¡å‹åç¨±ï¼ˆç”¨æ–¼é¸æ“‡å°æ‡‰çš„ tokenizerï¼‰
        :param chunk_size: [ç›®æ¨™å¤§å°] æ¯ä¸€æ®µæœŸæœ›çš„ Token æ•¸é‡é™åˆ¶ã€‚
        :param overlap: [é‡è¤‡å€é–“] ç›¸é„°å…©æ®µä¹‹é–“ã€Œé‡ç–Šã€çš„éƒ¨åˆ†ã€‚
                        ğŸ’¡ è§£é‡‹ï¼šé€™ä¸æ˜¯ 150/2 åˆ†åœ¨å‰å¾Œï¼Œè€Œæ˜¯ã€Œä¸‹ä¸€æ®µçš„å‰ X å€‹å­—ã€æœƒåŒ…å«ã€Œå‰ä¸€æ®µæœ€å¾Œçš„ X å€‹å­—ã€ã€‚
                        è¼ƒå¤§çš„ overlap (å¦‚ 300) èƒ½è®“ RAG åœ¨æª¢ç´¢åˆ°ç‰‡æ®µæ™‚ï¼Œä¿ç•™æ›´å¤šä¸Šä¸‹æ–‡é€£è²«æ€§ã€‚
        :param tolerance: [å¯¬å®¹ç·©è¡] å¦‚æœæœ€å¾Œä¸€æ®µåªå‰©ä¸‹ä¸€é»é»ï¼ˆToken æ•¸ < chunk_size + toleranceï¼‰ï¼Œ
                          å°±ç›´æ¥åˆä½µï¼Œä¸å†åˆ‡åˆ†ï¼Œé¿å…å‡ºç¾æ¥µçŸ­ç‰‡æ®µã€‚
        :param debug: æ˜¯å¦å•Ÿç”¨èª¿è©¦æ¨¡å¼ï¼ˆè¼¸å‡ºè©³ç´°æ—¥èªŒï¼‰
        :param table_aware: æ˜¯å¦å•Ÿç”¨è¡¨æ ¼æ„ŸçŸ¥æ¨¡å¼ï¼ˆé¿å…åœ¨è¡¨æ ¼ row ä¸­é–“åˆ‡æ–·ï¼‰
        """
        self.chunk_size = chunk_size
        self.overlap = overlap
        self.tolerance = tolerance
        self.debug = debug
        self.table_aware = table_aware

        # æ”¹é€²ç•°å¸¸è™•ç†ï¼šåªæ•ç²ç‰¹å®šç•°å¸¸
        try:
            self.enc = tiktoken.encoding_for_model(model_name)
        except (KeyError, ValueError) as e:
            if self.debug:
                print(
                    f"âš ï¸ ç„¡æ³•è¼‰å…¥æ¨¡å‹ {model_name} çš„ tokenizerï¼Œä½¿ç”¨é è¨­ cl100k_base: {e}"
                )
            self.enc = tiktoken.get_encoding("cl100k_base")

        # æ¨™é»å„ªå…ˆç´šï¼šæ›è¡Œ > å¥æœ«æ¨™é» > åˆ†è™Ÿ/å†’è™Ÿ > é€—è™Ÿ/é “è™Ÿ
        # æ“´å±•ä¸­æ–‡æ¨™é»ç¬¦è™Ÿæ”¯æ´
        self.separators = [
            "\n\n",
            "\n",
            "\r\n",  # Windows æ›è¡Œ
            "ã€‚",
            "ï¼",
            "ï¼Ÿ",
            "!",
            "?",
            "ï¼›",
            ";",
            "ï¼š",  # ä¸­æ–‡å†’è™Ÿ
            ":",  # è‹±æ–‡å†’è™Ÿ
            "ï¼Œ",
            ",",
            "ã€",  # ä¸­æ–‡é “è™Ÿ
        ]

    def _is_table_row(self, line: str) -> bool:
        """æª¢æŸ¥æ˜¯å¦ç‚º Markdown è¡¨æ ¼è¡Œ"""
        stripped = line.strip()
        return (
            stripped.startswith("|")
            and stripped.endswith("|")
            and stripped.count("|") >= 2
        )

    def _find_table_boundary(
        self, text: str, start_pos: int, direction: str = "backward"
    ) -> int:
        """
        å°‹æ‰¾è¡¨æ ¼é‚Šç•Œ

        :param text: å®Œæ•´æ–‡æœ¬
        :param start_pos: èµ·å§‹ä½ç½®
        :param direction: 'backward' å‘å‰æ‰¾è¡¨æ ¼é–‹å§‹ï¼Œ'forward' å‘å¾Œæ‰¾è¡¨æ ¼çµæŸ
        :return: è¡¨æ ¼é‚Šç•Œä½ç½®
        """
        lines = (
            text[:start_pos].split("\n")
            if direction == "backward"
            else text[start_pos:].split("\n")
        )

        if direction == "backward":
            # å‘å‰æ‰¾ï¼šæ‰¾åˆ°ç¬¬ä¸€å€‹éè¡¨æ ¼è¡Œ
            for i in range(len(lines) - 1, -1, -1):
                if not self._is_table_row(lines[i]):
                    # è¿”å›é€™å€‹éè¡¨æ ¼è¡Œä¹‹å¾Œçš„ä½ç½®
                    boundary = sum(len(lines[j]) + 1 for j in range(i + 1))  # +1 for \n
                    return boundary
            return 0  # æ•´å€‹éƒ½æ˜¯è¡¨æ ¼
        else:
            # å‘å¾Œæ‰¾ï¼šæ‰¾åˆ°ç¬¬ä¸€å€‹éè¡¨æ ¼è¡Œ
            for i, line in enumerate(lines):
                if not self._is_table_row(line):
                    # è¿”å›é€™å€‹éè¡¨æ ¼è¡Œä¹‹å‰çš„ä½ç½®
                    boundary = start_pos + sum(len(lines[j]) + 1 for j in range(i))
                    return boundary
            return len(text)  # æ•´å€‹éƒ½æ˜¯è¡¨æ ¼

    def count_tokens(self, text: str) -> int:
        if not text:
            return 0
        return len(self.enc.encode(text))

    def split_text(self, text: str) -> List[str]:
        """ä¸»åˆ‡åˆ†é‚è¼¯ï¼šæ”¹ç‚ºè¿­ä»£å¼ï¼Œè™•ç†é•·æ–‡æ›´ç©©å®š"""
        total_tokens = self.count_tokens(text)

        # ç·©è¡æª¢æŸ¥ï¼šå¦‚æœç¸½é•·åº¦åœ¨ (ç›®æ¨™ + å¯¬å®¹å€¼) å…§ï¼Œç›´æ¥å›å‚³
        if total_tokens <= (self.chunk_size + self.tolerance):
            return [text.strip()] if text.strip() else []

        chunks = []
        start_char_idx = 0
        text_len = len(text)

        while start_char_idx < text_len:
            # 1. ä¼°ç®—ç•¶å‰ç‰‡æ®µçš„çµæŸå­—å…ƒä½ç½®
            remaining_text = text[start_char_idx:]
            remaining_tokens_ids = self.enc.encode(remaining_text)

            # å¦‚æœå‰©ä¸‹ä¸é•·äº†ï¼Œç›´æ¥å…¨åŒ…
            if len(remaining_tokens_ids) <= (self.chunk_size + self.tolerance):
                chunks.append(remaining_text.strip())
                break

            # 2. å®šä½ã€Œç¡¬æ€§ä¸Šé™ã€åˆ‡å‰²é» (æ ¹æ“š chunk_size)
            hard_limit_ids = remaining_tokens_ids[: self.chunk_size]
            hard_limit_char_len = len(self.enc.decode(hard_limit_ids))
            current_end_boundary = start_char_idx + hard_limit_char_len

            # 3. æ™ºæ…§å°‹æ‰¾ã€Œåˆ‡å‰²é»ã€ï¼šåœ¨çµå°¾é™„è¿‘æ‰¾æ¨™é»ç¬¦è™Ÿï¼Œè®“åˆ‡å‰²æ›´è‡ªç„¶
            search_range = 150
            search_start = max(start_char_idx, current_end_boundary - search_range)
            snippet = text[search_start:current_end_boundary]

            best_split_point = current_end_boundary
            for sep in self.separators:
                found_idx = snippet.rfind(sep)
                if found_idx != -1:
                    best_split_point = search_start + found_idx + len(sep)
                    break

            current_chunk = text[start_char_idx:best_split_point].strip()
            if current_chunk:
                chunks.append(current_chunk)

            # 4. è¨ˆç®—ã€Œä¸‹ä¸€æ®µã€çš„èµ·å§‹é» (è™•ç† Overlap)
            tokens_in_chunk = self.enc.encode(text[start_char_idx:best_split_point])
            overlap_token_count = min(self.overlap, len(tokens_in_chunk))
            overlap_ids = tokens_in_chunk[-overlap_token_count:]
            overlap_char_len = len(self.enc.decode(overlap_ids))

            theoretical_next_start = best_split_point - overlap_char_len

            # --- æ™ºæ…§èµ·å§‹é»å°‹æ‰¾ (Smart Start) ---
            s_min = max(start_char_idx + 1, theoretical_next_start - 50)
            s_max = min(best_split_point - 1, theoretical_next_start + 50)

            best_next_start = theoretical_next_start
            if s_max > s_min:
                start_snippet = text[s_min:s_max]
                for sep in self.separators:
                    found_idx = start_snippet.find(sep)
                    if found_idx != -1:
                        best_next_start = s_min + found_idx + len(sep)
                        break

            # é˜²å‘†ï¼šé¿å…åŸåœ°è¸æ­¥ï¼ˆå¢å¼·ç‰ˆå®‰å…¨æª¢æŸ¥ï¼‰
            if best_next_start <= start_char_idx:
                # ç¢ºä¿è‡³å°‘å‰é€²ä¸€äº›è·é›¢ï¼Œé¿å…ç„¡é™å¾ªç’°
                start_char_idx = max(best_split_point, start_char_idx + 1)
            else:
                start_char_idx = best_next_start

            # é¡å¤–å®‰å…¨æª¢æŸ¥ï¼šå¦‚æœä¸‹ä¸€å€‹èµ·å§‹é»è¶…éæ–‡æœ¬é•·åº¦ï¼Œç›´æ¥çµæŸ
            if start_char_idx >= text_len:
                break

        return chunks

    def split_text_optimized(self, text: str) -> List[str]:
        """
        å„ªåŒ–ç‰ˆåˆ‡åˆ†é‚è¼¯ï¼šä½¿ç”¨å¿«å–æ©Ÿåˆ¶æ¸›å°‘é‡è¤‡ç·¨ç¢¼

        ä¸»è¦æ”¹é€²ï¼š
        1. ä¸€æ¬¡æ€§ç·¨ç¢¼æ•´å€‹æ–‡æœ¬ä¸¦å¿«å–
        2. å»ºç«‹ token-char æ˜ å°„è¡¨æé«˜ç²¾åº¦
        3. æ¸›å°‘é‡è¤‡çš„ç·¨ç¢¼/è§£ç¢¼æ“ä½œ
        4. æ›´å®‰å…¨çš„é‚Šç•Œæª¢æŸ¥
        """
        if not text or not text.strip():
            return []

        # ä¸€æ¬¡æ€§ç·¨ç¢¼æ•´å€‹æ–‡æœ¬ï¼ˆå¿«å–ï¼‰
        all_token_ids = self.enc.encode(text)
        total_tokens = len(all_token_ids)

        if self.debug:
            print(f"ğŸ“Š æ–‡æœ¬ç¸½é•·åº¦: {len(text)} å­—å…ƒ, {total_tokens} tokens")

        # ç·©è¡æª¢æŸ¥ï¼šå¦‚æœç¸½é•·åº¦åœ¨ (ç›®æ¨™ + å¯¬å®¹å€¼) å…§ï¼Œç›´æ¥å›å‚³
        if total_tokens <= (self.chunk_size + self.tolerance):
            if self.debug:
                print(f"âœ… æ–‡æœ¬é•·åº¦åœ¨å®¹è¨±ç¯„åœå…§ï¼Œä¸éœ€åˆ‡åˆ†")
            return [text.strip()]

        chunks = []
        token_start_idx = 0

        while token_start_idx < total_tokens:
            # 1. è¨ˆç®—ç•¶å‰ chunk çš„ token ç¯„åœ
            token_end_idx = min(token_start_idx + self.chunk_size, total_tokens)

            # å¦‚æœå‰©é¤˜ tokens ä¸å¤šï¼Œç›´æ¥å…¨åŒ…
            remaining_tokens = total_tokens - token_start_idx
            if remaining_tokens <= (self.chunk_size + self.tolerance):
                chunk_token_ids = all_token_ids[token_start_idx:]
                chunk_text = self.enc.decode(chunk_token_ids).strip()
                if chunk_text:
                    chunks.append(chunk_text)
                    if self.debug:
                        print(
                            f"âœ… æœ€å¾Œä¸€æ®µ (tokens: {len(chunk_token_ids)}): {chunk_text[:50]}..."
                        )
                break

            # 2. è§£ç¢¼ç•¶å‰ chunk
            chunk_token_ids = all_token_ids[token_start_idx:token_end_idx]
            chunk_text = self.enc.decode(chunk_token_ids)

            # 3. æ™ºæ…§å°‹æ‰¾åˆ‡å‰²é»ï¼ˆåœ¨ chunk æœ«å°¾é™„è¿‘æ‰¾æ¨™é»ï¼‰
            search_range = min(150, len(chunk_text) // 2)  # å‹•æ…‹èª¿æ•´æœå°‹ç¯„åœ
            search_start = max(0, len(chunk_text) - search_range)
            snippet = chunk_text[search_start:]

            best_split_offset = len(chunk_text)  # é è¨­ï¼šæ•´æ®µ
            for sep in self.separators:
                found_idx = snippet.rfind(sep)
                if found_idx != -1:
                    best_split_offset = search_start + found_idx + len(sep)
                    break

            # åˆ‡å‰²æ–‡æœ¬
            final_chunk = chunk_text[:best_split_offset].strip()
            if final_chunk:
                chunks.append(final_chunk)
                if self.debug:
                    actual_tokens = self.count_tokens(final_chunk)
                    print(
                        f"ğŸ“ Chunk {len(chunks)} (tokens: {actual_tokens}): {final_chunk[:50]}..."
                    )

            # 4. è¨ˆç®—ä¸‹ä¸€æ®µçš„èµ·å§‹é»ï¼ˆè™•ç† overlapï¼‰
            # é‡æ–°ç·¨ç¢¼åˆ‡å‰²å¾Œçš„æ–‡æœ¬ä»¥ç²å¾—ç²¾ç¢ºçš„ token æ•¸
            final_chunk_tokens = self.enc.encode(final_chunk)
            overlap_token_count = min(self.overlap, len(final_chunk_tokens))

            # è¨˜éŒ„ç•¶å‰ä½ç½®ç”¨æ–¼å®‰å…¨æª¢æŸ¥
            prev_token_start = token_start_idx

            # ä¸‹ä¸€æ®µå¾ã€Œç•¶å‰æ®µ - overlapã€é–‹å§‹
            token_start_idx = (
                token_start_idx + len(final_chunk_tokens) - overlap_token_count
            )

            # å®‰å…¨æª¢æŸ¥ï¼šç¢ºä¿æœ‰å‰é€²
            if token_start_idx <= prev_token_start:
                token_start_idx = token_end_idx
                if self.debug:
                    print(f"âš ï¸ åµæ¸¬åˆ°å¯èƒ½çš„ç„¡é™å¾ªç’°ï¼Œå¼·åˆ¶å‰é€²")

        if self.debug:
            print(f"âœ… åˆ‡åˆ†å®Œæˆï¼Œå…± {len(chunks)} æ®µ")

        return chunks

    def split_text_table_aware(self, text: str) -> List[str]:
        """
        è¡¨æ ¼æ„ŸçŸ¥åˆ‡åˆ†ï¼šç¢ºä¿ä¸æœƒåœ¨è¡¨æ ¼ row ä¸­é–“åˆ‡æ–·

        ç­–ç•¥ï¼š
        1. ä½¿ç”¨å„ªåŒ–ç‰ˆåˆ‡åˆ†é‚è¼¯
        2. åœ¨åˆ‡åˆ†é»æª¢æŸ¥æ˜¯å¦ä½æ–¼è¡¨æ ¼å…§
        3. å¦‚æœåœ¨è¡¨æ ¼å…§ï¼Œèª¿æ•´åˆ°è¡¨æ ¼é‚Šç•Œ
        4. ä¿ç•™è¡¨æ ¼æ¨™é¡Œå’Œåˆ†éš”ç·š
        """
        if not text or not text.strip():
            return []

        # ä¸€æ¬¡æ€§ç·¨ç¢¼æ•´å€‹æ–‡æœ¬ï¼ˆå¿«å–ï¼‰
        all_token_ids = self.enc.encode(text)
        total_tokens = len(all_token_ids)

        if self.debug:
            print(f"ğŸ“Š æ–‡æœ¬ç¸½é•·åº¦: {len(text)} å­—å…ƒ, {total_tokens} tokens")

        # ç·©è¡æª¢æŸ¥ï¼šå¦‚æœç¸½é•·åº¦åœ¨ (ç›®æ¨™ + å¯¬å®¹å€¼) å…§ï¼Œç›´æ¥å›å‚³
        if total_tokens <= (self.chunk_size + self.tolerance):
            if self.debug:
                print(f"âœ… æ–‡æœ¬é•·åº¦åœ¨å®¹è¨±ç¯„åœå…§ï¼Œä¸éœ€åˆ‡åˆ†")
            return [text.strip()]

        chunks = []
        char_start_idx = 0
        text_len = len(text)

        # ç”¨æ–¼å„²å­˜è¡¨æ ¼æ¨™é¡Œå’Œåˆ†éš”ç·šï¼ˆå¦‚æœéœ€è¦é‡è¤‡ä½¿ç”¨ï¼‰
        table_header_cache = {}

        while char_start_idx < text_len:
            # 1. ä¼°ç®—ç•¶å‰ chunk çš„å­—å…ƒç¯„åœï¼ˆåŸºæ–¼ token æ•¸ï¼‰
            remaining_text = text[char_start_idx:]
            remaining_tokens_ids = self.enc.encode(remaining_text)

            # å¦‚æœå‰©é¤˜ tokens ä¸å¤šï¼Œç›´æ¥å…¨åŒ…
            remaining_tokens = len(remaining_tokens_ids)
            if remaining_tokens <= (self.chunk_size + self.tolerance):
                chunk_text = remaining_text.strip()
                if chunk_text:
                    chunks.append(chunk_text)
                    if self.debug:
                        print(
                            f"âœ… æœ€å¾Œä¸€æ®µ (tokens: {remaining_tokens}): {chunk_text[:50]}..."
                        )
                break

            # 2. å®šä½ã€Œç¡¬æ€§ä¸Šé™ã€åˆ‡å‰²é»
            hard_limit_ids = remaining_tokens_ids[: self.chunk_size]
            hard_limit_text = self.enc.decode(hard_limit_ids)
            hard_limit_char_len = len(hard_limit_text)
            current_end_boundary = char_start_idx + hard_limit_char_len

            # 3. æ™ºæ…§å°‹æ‰¾åˆ‡å‰²é»ï¼ˆæ¨™é»ç¬¦è™Ÿï¼‰
            search_range = min(150, hard_limit_char_len // 2)
            search_start = max(char_start_idx, current_end_boundary - search_range)
            snippet = text[search_start:current_end_boundary]

            best_split_point = current_end_boundary
            for sep in self.separators:
                found_idx = snippet.rfind(sep)
                if found_idx != -1:
                    best_split_point = search_start + found_idx + len(sep)
                    break

            # 4. è¡¨æ ¼æ„ŸçŸ¥èª¿æ•´
            if self.table_aware:
                # æª¢æŸ¥åˆ‡åˆ†é»é™„è¿‘æ˜¯å¦æœ‰è¡¨æ ¼
                context_before = text[max(0, best_split_point - 200) : best_split_point]
                context_after = text[
                    best_split_point : min(text_len, best_split_point + 200)
                ]

                # æª¢æŸ¥åˆ‡åˆ†é»å‰å¾Œæ˜¯å¦æœ‰è¡¨æ ¼è¡Œ
                lines_before = context_before.split("\n")
                lines_after = context_after.split("\n")

                in_table = False
                if lines_before and self._is_table_row(lines_before[-1]):
                    in_table = True
                if lines_after and self._is_table_row(lines_after[0]):
                    in_table = True

                if in_table:
                    if self.debug:
                        print(f"âš ï¸  åµæ¸¬åˆ°è¡¨æ ¼ï¼Œèª¿æ•´åˆ‡åˆ†é»...")

                    # å‘å¾Œæ‰¾åˆ°è¡¨æ ¼çµæŸ
                    table_end = best_split_point
                    for i in range(
                        best_split_point, min(text_len, best_split_point + 500)
                    ):
                        if text[i] == "\n":
                            next_line_start = i + 1
                            next_line_end = text.find("\n", next_line_start)
                            if next_line_end == -1:
                                next_line_end = text_len
                            next_line = text[next_line_start:next_line_end]

                            if not self._is_table_row(next_line):
                                table_end = i + 1  # åœ¨è¡¨æ ¼å¾Œçš„æ›è¡Œç¬¦ä¹‹å¾Œåˆ‡åˆ†
                                break

                    # æª¢æŸ¥èª¿æ•´å¾Œçš„å¤§å°æ˜¯å¦å¯æ¥å—
                    adjusted_chunk = text[char_start_idx:table_end]
                    adjusted_tokens = self.count_tokens(adjusted_chunk)

                    if adjusted_tokens <= (self.chunk_size + self.tolerance):
                        best_split_point = table_end
                        if self.debug:
                            print(f"   âœ… èª¿æ•´åˆ°è¡¨æ ¼çµæŸ (tokens: {adjusted_tokens})")
                    else:
                        # å¦‚æœèª¿æ•´å¾Œå¤ªå¤§ï¼Œå‘å‰æ‰¾è¡¨æ ¼é–‹å§‹
                        table_start = char_start_idx
                        for i in range(best_split_point - 1, char_start_idx, -1):
                            if text[i] == "\n":
                                prev_line_end = i
                                prev_line_start = (
                                    text.rfind("\n", char_start_idx, prev_line_end) + 1
                                )
                                prev_line = text[prev_line_start:prev_line_end]

                                if not self._is_table_row(prev_line):
                                    table_start = prev_line_end + 1
                                    break

                        best_split_point = table_start
                        if self.debug:
                            print(f"   âš ï¸  è¡¨æ ¼å¤ªå¤§ï¼Œèª¿æ•´åˆ°è¡¨æ ¼é–‹å§‹")

            # 5. æå–ç•¶å‰ chunk
            current_chunk = text[char_start_idx:best_split_point].strip()
            if current_chunk:
                chunks.append(current_chunk)
                if self.debug:
                    actual_tokens = self.count_tokens(current_chunk)
                    print(
                        f"ğŸ“ Chunk {len(chunks)} (tokens: {actual_tokens}): {current_chunk[:50]}..."
                    )

            # 6. è¨ˆç®—ä¸‹ä¸€æ®µçš„èµ·å§‹é»ï¼ˆè™•ç† overlapï¼‰
            chunk_tokens = self.enc.encode(current_chunk)
            overlap_token_count = min(self.overlap, len(chunk_tokens))

            if overlap_token_count > 0:
                overlap_ids = chunk_tokens[-overlap_token_count:]
                overlap_text = self.enc.decode(overlap_ids)
                overlap_char_len = len(overlap_text)
                char_start_idx = best_split_point - overlap_char_len
            else:
                char_start_idx = best_split_point

            # å®‰å…¨æª¢æŸ¥ï¼šç¢ºä¿æœ‰å‰é€²
            if char_start_idx >= text_len:
                break

        if self.debug:
            print(f"âœ… åˆ‡åˆ†å®Œæˆï¼Œå…± {len(chunks)} æ®µ")

        return chunks

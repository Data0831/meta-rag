import tiktoken
from typing import List, Optional
from config.config import TokenConfig


class UnifiedTokenSplitter:
    def __init__(
        self,
        model_name: str = TokenConfig.MODEL_NAME,
        chunk_size: int = TokenConfig.CHUNK_SIZE,
        overlap: int = TokenConfig.OVERLAP,
        tolerance: int = TokenConfig.TOLERANCE,
        debug: bool = False,
        table_aware: bool = True,
    ):
        """
        æ··åˆç‰ˆç²¾æº–åˆ‡åˆ†å™¨ï¼šçµåˆ Token è¨ˆæ•¸èˆ‡æ™ºæ…§æ¨™é»è­˜åˆ¥ã€‚

        åƒæ•¸èªªæ˜ï¼š
        :param model_name: ä½¿ç”¨çš„æ¨¡å‹åç¨±ï¼ˆç”¨æ–¼é¸æ“‡å°æ‡‰çš„ tokenizerï¼‰
        :param chunk_size: [ç›®æ¨™å¤§å°] æ¯ä¸€æ®µæœŸæœ›çš„ Token æ•¸é‡é™åˆ¶ã€‚
        :param overlap: [é‡è¤‡å€é–“] ç›¸é„°å…©æ®µä¹‹é–“ã€Œé‡ç–Šã€çš„éƒ¨åˆ†ã€‚
                        ğŸ’¡ è§£é‡‹ï¼šé€™ä¸æ˜¯ 150/2 åˆ†åœ¨å‰å¾Œï¼Œè€Œæ˜¯ã€Œä¸‹ä¸€æ®µçš„å‰ X å€‹å­—ã€æœƒåŒ…å«ã€Œå‰ä¸€æ®µæœ€å¾Œçš„ X å€‹å­—ã€ã€‚
                        è¼ƒå¤§çš„ overlap (å¦‚ 300) èƒ½è®“ RAG åœ¨æª¢ç´¢åˆ°ç‰‡æ®µæ™‚ï¼Œä¿ç•™æ›´å¤šä¸Šä¸‹æ–‡é€£è²«æ€§ã€‚
        :param tolerance: [å¯¬å®¹ç·©è¡] å¦‚æœæœ€å¾Œä¸€æ®µåªå‰©ä¸‹ä¸€é»é»ï¼ˆToken æ•¸ < chunk_size + toleranceï¼‰ï¼Œ
                          å°±ç›´æ¥åˆä½µï¼Œä¸å†åˆ‡åˆ†ï¼Œé¿å…å‡ºç¾æ¥µçŸ­ç‰‡æ®µã€‚
        :param debug: æ˜¯å¦å•Ÿç”¨èª¿è©¦æ¨¡å¼ï¼ˆè¼¸å‡ºè©³ç´°æ—¥èªŒï¼‰
        :param table_aware: æ˜¯å¦å•Ÿç”¨è¡¨æ ¼æ„ŸçŸ¥æ¨¡å¼ï¼ˆé¿å…åœ¨è¡¨æ ¼ row ä¸­é–“åˆ‡æ–·ï¼‰
        """
        self.chunk_size = chunk_size
        self.overlap = overlap
        self.tolerance = tolerance
        self.debug = debug
        self.table_aware = table_aware

        # æ”¹é€²ç•°å¸¸è™•ç†ï¼šåªæ•ç²ç‰¹å®šç•°å¸¸
        try:
            self.enc = tiktoken.encoding_for_model(model_name)
        except (KeyError, ValueError) as e:
            if self.debug:
                print(
                    f"âš ï¸ ç„¡æ³•è¼‰å…¥æ¨¡å‹ {model_name} çš„ tokenizerï¼Œä½¿ç”¨é è¨­ cl100k_base: {e}"
                )
            self.enc = tiktoken.get_encoding("cl100k_base")

        # æ¨™é»å„ªå…ˆç´šï¼šæ›è¡Œ > å¥æœ«æ¨™é» > åˆ†è™Ÿ/å†’è™Ÿ > é€—è™Ÿ/é “è™Ÿ
        # æ“´å±•ä¸­æ–‡æ¨™é»ç¬¦è™Ÿæ”¯æ´
        self.separators = [
            "\n\n",
            "\n",
            "\r\n",  # Windows æ›è¡Œ
            "ã€‚",
            "ï¼",
            "ï¼Ÿ",
            "!",
            "?",
            "ï¼›",
            ";",
            "ï¼š",  # ä¸­æ–‡å†’è™Ÿ
            ":",  # è‹±æ–‡å†’è™Ÿ
            "ï¼Œ",
            ",",
            "ã€",  # ä¸­æ–‡é “è™Ÿ
        ]

    def _is_table_row(self, line: str) -> bool:
        """æª¢æŸ¥æ˜¯å¦ç‚º Markdown è¡¨æ ¼è¡Œ"""
        stripped = line.strip()
        return (
            stripped.startswith("|")
            and stripped.endswith("|")
            and stripped.count("|") >= 2
        )

    def _find_table_boundary(
        self, text: str, start_pos: int, direction: str = "backward"
    ) -> int:
        """
        å°‹æ‰¾è¡¨æ ¼é‚Šç•Œ

        :param text: å®Œæ•´æ–‡æœ¬
        :param start_pos: èµ·å§‹ä½ç½®
        :param direction: 'backward' å‘å‰æ‰¾è¡¨æ ¼é–‹å§‹ï¼Œ'forward' å‘å¾Œæ‰¾è¡¨æ ¼çµæŸ
        :return: è¡¨æ ¼é‚Šç•Œä½ç½®
        """
        lines = (
            text[:start_pos].split("\n")
            if direction == "backward"
            else text[start_pos:].split("\n")
        )

        if direction == "backward":
            # å‘å‰æ‰¾ï¼šæ‰¾åˆ°ç¬¬ä¸€å€‹éè¡¨æ ¼è¡Œ
            for i in range(len(lines) - 1, -1, -1):
                if not self._is_table_row(lines[i]):
                    # è¿”å›é€™å€‹éè¡¨æ ¼è¡Œä¹‹å¾Œçš„ä½ç½®
                    boundary = sum(len(lines[j]) + 1 for j in range(i + 1))  # +1 for \n
                    return boundary
            return 0  # æ•´å€‹éƒ½æ˜¯è¡¨æ ¼
        else:
            # å‘å¾Œæ‰¾ï¼šæ‰¾åˆ°ç¬¬ä¸€å€‹éè¡¨æ ¼è¡Œ
            for i, line in enumerate(lines):
                if not self._is_table_row(line):
                    # è¿”å›é€™å€‹éè¡¨æ ¼è¡Œä¹‹å‰çš„ä½ç½®
                    boundary = start_pos + sum(len(lines[j]) + 1 for j in range(i))
                    return boundary
            return len(text)  # æ•´å€‹éƒ½æ˜¯è¡¨æ ¼

    def count_tokens(self, text: str) -> int:
        if not text:
            return 0
        return len(self.enc.encode(text))

    def split_text(self, text: str) -> List[str]:
        """
        çµ±ä¸€çš„æ–‡æœ¬åˆ‡åˆ†æ–¹æ³•

        :param text: è¦åˆ‡åˆ†çš„æ–‡æœ¬
        :return: åˆ‡åˆ†å¾Œçš„æ–‡æœ¬ç‰‡æ®µåˆ—è¡¨
        """
        if not text or not text.strip():
            return []

        total_tokens = self.count_tokens(text)

        if self.debug:
            print(f"ğŸ“Š æ–‡æœ¬ç¸½é•·åº¦: {len(text)} å­—å…ƒ, {total_tokens} tokens")
            print(f"ğŸ”§ è¡¨æ ¼æ„ŸçŸ¥æ¨¡å¼: {'å•Ÿç”¨' if self.table_aware else 'åœç”¨'}")

        # ç·©è¡æª¢æŸ¥ï¼šå¦‚æœç¸½é•·åº¦åœ¨ (ç›®æ¨™ + å¯¬å®¹å€¼) å…§ï¼Œç›´æ¥å›å‚³
        if total_tokens <= (self.chunk_size + self.tolerance):
            if self.debug:
                print(f"âœ… æ–‡æœ¬é•·åº¦åœ¨å®¹è¨±ç¯„åœå…§ï¼Œä¸éœ€åˆ‡åˆ†")
            return [text.strip()]

        chunks = []
        char_start_idx = 0
        text_len = len(text)

        while char_start_idx < text_len:
            # 1. ä¼°ç®—ç•¶å‰ç‰‡æ®µçš„çµæŸå­—å…ƒä½ç½®
            remaining_text = text[char_start_idx:]
            remaining_tokens_ids = self.enc.encode(remaining_text)

            # å¦‚æœå‰©é¤˜ tokens ä¸å¤šï¼Œç›´æ¥å…¨åŒ…
            remaining_tokens = len(remaining_tokens_ids)
            if remaining_tokens <= (self.chunk_size + self.tolerance):
                chunk_text = remaining_text.strip()
                if chunk_text:
                    chunks.append(chunk_text)
                    if self.debug:
                        print(
                            f"âœ… æœ€å¾Œä¸€æ®µ (tokens: {remaining_tokens}): {chunk_text[:50]}..."
                        )
                break

            # 2. å®šä½ã€Œç¡¬æ€§ä¸Šé™ã€åˆ‡å‰²é» (æ ¹æ“š chunk_size)
            hard_limit_ids = remaining_tokens_ids[: self.chunk_size]
            hard_limit_text = self.enc.decode(hard_limit_ids)
            hard_limit_char_len = len(hard_limit_text)
            current_end_boundary = char_start_idx + hard_limit_char_len

            # 3. æ™ºæ…§å°‹æ‰¾åˆ‡å‰²é»ï¼ˆæ¨™é»ç¬¦è™Ÿï¼‰
            # æœå°‹ç¯„åœå‹•æ…‹èª¿æ•´ç‚º chunk_size çš„ 1/4ï¼Œæœ€å°ç‚º chunk_size çš„ 1/20ï¼Œä½†ä¸è¶…éå¯¦éš›æ–‡æœ¬é•·åº¦çš„ä¸€åŠ
            search_range = min(
                max(self.chunk_size // 20, self.chunk_size // 4),
                hard_limit_char_len // 2,
            )
            search_start = max(char_start_idx, current_end_boundary - search_range)
            snippet = text[search_start:current_end_boundary]

            best_split_point = current_end_boundary
            for sep in self.separators:
                found_idx = snippet.rfind(sep)
                if found_idx != -1:
                    best_split_point = search_start + found_idx + len(sep)
                    break

            # 4. è¡¨æ ¼æ„ŸçŸ¥èª¿æ•´
            if self.table_aware:
                best_split_point = self._adjust_split_for_table(
                    text, char_start_idx, best_split_point, text_len
                )

            # 5. æå–ç•¶å‰ chunk
            current_chunk = text[char_start_idx:best_split_point].strip()
            if current_chunk:
                chunks.append(current_chunk)
                if self.debug:
                    actual_tokens = self.count_tokens(current_chunk)
                    print(
                        f"ğŸ“ Chunk {len(chunks)} (tokens: {actual_tokens}): {current_chunk[:50]}..."
                    )

            # 6. è¨ˆç®—ä¸‹ä¸€æ®µçš„èµ·å§‹é»ï¼ˆè™•ç† overlapï¼‰
            chunk_tokens = self.enc.encode(current_chunk)
            overlap_token_count = min(self.overlap, len(chunk_tokens))

            if overlap_token_count > 0:
                overlap_ids = chunk_tokens[-overlap_token_count:]
                overlap_text = self.enc.decode(overlap_ids)
                overlap_char_len = len(overlap_text)
                char_start_idx = best_split_point - overlap_char_len
            else:
                char_start_idx = best_split_point

            # å®‰å…¨æª¢æŸ¥ï¼šç¢ºä¿æœ‰å‰é€²
            if char_start_idx >= text_len:
                break

        if self.debug:
            print(f"âœ… åˆ‡åˆ†å®Œæˆï¼Œå…± {len(chunks)} æ®µ")

        return chunks

    def _adjust_split_for_table(
        self, text: str, start_idx: int, split_point: int, text_len: int
    ) -> int:
        """
        èª¿æ•´åˆ‡åˆ†é»ä»¥é¿å…åœ¨è¡¨æ ¼ä¸­é–“åˆ‡æ–·

        :param text: å®Œæ•´æ–‡æœ¬
        :param start_idx: ç•¶å‰æ®µè½èµ·å§‹ä½ç½®
        :param split_point: å»ºè­°çš„åˆ‡åˆ†é»
        :param text_len: æ–‡æœ¬ç¸½é•·åº¦
        :return: èª¿æ•´å¾Œçš„åˆ‡åˆ†é»
        """
        # æª¢æŸ¥åˆ‡åˆ†é»é™„è¿‘æ˜¯å¦æœ‰è¡¨æ ¼
        context_before = text[max(0, split_point - 200) : split_point]
        context_after = text[split_point : min(text_len, split_point + 200)]

        # æª¢æŸ¥åˆ‡åˆ†é»å‰å¾Œæ˜¯å¦æœ‰è¡¨æ ¼è¡Œ
        lines_before = context_before.split("\n")
        lines_after = context_after.split("\n")

        in_table = False
        if lines_before and self._is_table_row(lines_before[-1]):
            in_table = True
        if lines_after and self._is_table_row(lines_after[0]):
            in_table = True

        if not in_table:
            return split_point

        if self.debug:
            print(f"âš ï¸  åµæ¸¬åˆ°è¡¨æ ¼ï¼Œèª¿æ•´åˆ‡åˆ†é»...")

        # å‘å¾Œæ‰¾åˆ°è¡¨æ ¼çµæŸ
        table_end = split_point
        for i in range(split_point, min(text_len, split_point + 500)):
            if text[i] == "\n":
                next_line_start = i + 1
                next_line_end = text.find("\n", next_line_start)
                if next_line_end == -1:
                    next_line_end = text_len
                next_line = text[next_line_start:next_line_end]

                if not self._is_table_row(next_line):
                    table_end = i + 1  # åœ¨è¡¨æ ¼å¾Œçš„æ›è¡Œç¬¦ä¹‹å¾Œåˆ‡åˆ†
                    break

        # æª¢æŸ¥èª¿æ•´å¾Œçš„å¤§å°æ˜¯å¦å¯æ¥å—
        adjusted_chunk = text[start_idx:table_end]
        adjusted_tokens = self.count_tokens(adjusted_chunk)

        if adjusted_tokens <= (self.chunk_size + self.tolerance):
            if self.debug:
                print(f"   âœ… èª¿æ•´åˆ°è¡¨æ ¼çµæŸ (tokens: {adjusted_tokens})")
            return table_end
        else:
            # å¦‚æœèª¿æ•´å¾Œå¤ªå¤§ï¼Œå‘å‰æ‰¾è¡¨æ ¼é–‹å§‹
            table_start = start_idx
            for i in range(split_point - 1, start_idx, -1):
                if text[i] == "\n":
                    prev_line_end = i
                    prev_line_start = text.rfind("\n", start_idx, prev_line_end) + 1
                    prev_line = text[prev_line_start:prev_line_end]

                    if not self._is_table_row(prev_line):
                        table_start = prev_line_end + 1
                        break

            if self.debug:
                print(f"   âš ï¸  è¡¨æ ¼å¤ªå¤§ï¼Œèª¿æ•´åˆ°è¡¨æ ¼é–‹å§‹")
            return table_start

    def split_text_optimized(self, text: str) -> List[str]:
        """
        å„ªåŒ–ç‰ˆåˆ‡åˆ†é‚è¼¯ï¼ˆå‘å¾Œå…¼å®¹åˆ¥åï¼‰

        æ³¨æ„ï¼šæ­¤æ–¹æ³•ç¾åœ¨ç›´æ¥èª¿ç”¨ split_textï¼Œä¸å†æœ‰ç¨ç«‹å¯¦ç¾ã€‚
        åŸæœ¬çš„ã€Œå„ªåŒ–ã€ï¼ˆåŸºæ–¼ token ç´¢å¼•ï¼‰å·²æ•´åˆåˆ°ä¸»æ–¹æ³•ä¸­ã€‚
        """
        return self.split_text(text)

    def split_text_table_aware(self, text: str) -> List[str]:
        """
        è¡¨æ ¼æ„ŸçŸ¥åˆ‡åˆ†ï¼ˆå‘å¾Œå…¼å®¹åˆ¥åï¼‰

        æ³¨æ„ï¼šæ­¤æ–¹æ³•ç¾åœ¨ç›´æ¥èª¿ç”¨ split_text ä¸¦å•Ÿç”¨è¡¨æ ¼æ„ŸçŸ¥æ¨¡å¼ã€‚
        """
        return self.split_text(text)

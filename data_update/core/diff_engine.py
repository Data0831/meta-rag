import os
import json
import hashlib
import tiktoken
import shutil
import glob
from datetime import datetime
from typing import List, Dict, Optional, Union

# ==========================================
# âš™ï¸ è¨­å®šå€
# ==========================================
DATA_DIR = "data"
HISTORY_DIR = os.path.join(DATA_DIR, "history")

# Token é™åˆ¶ (åƒ…ä½œæª¢æŸ¥ç”¨ï¼Œä¸å¼·åˆ¶æˆªæ–·ï¼Œé¿å…ç ´å£ JSON çµæ§‹)
TOKEN_LIMIT = 1800

# æ­·å²å‚™ä»½ä¿ç•™ä»½æ•¸
MAX_HISTORY_COUNT = 4

# åˆå§‹åŒ– Token è¨ˆç®—å™¨
try:
    enc = tiktoken.encoding_for_model("gpt-4o")
except:
    enc = tiktoken.get_encoding("cl100k_base")

# ==========================================
# ğŸ› ï¸ å·¥å…·å‡½å¼
# ==========================================

def count_tokens(text: str) -> int:
    """è¨ˆç®—å­—ä¸²çš„ Token æ•¸é‡"""
    return len(enc.encode(text))

def calculate_chunk_fingerprint(chunk: Dict) -> str:
    """
    è¨ˆç®—æŒ‡ç´‹ (Hash ID)ã€‚
    é‚è¼¯: website + main_title + title + content
    ğŸ”¥ åŠ å…¥ website æ˜¯ç‚ºäº†ç¢ºä¿è·¨ä¾†æº ID çš„å…¨åŸŸå”¯ä¸€æ€§ã€‚
    """
    website = str(chunk.get("website", "")).strip()
    main_title = str(chunk.get("main_title", "")).strip()
    title = str(chunk.get("title", "")).strip()
    content = str(chunk.get("content", "")).strip()
    
    # çµ„åˆå­—ä¸²é€²è¡Œé›œæ¹Š
    combined = f"{website}|{main_title}|{title}|{content}"
    return hashlib.md5(combined.encode('utf-8')).hexdigest()

def archive_old_file(source_name: str, file_path: str):
    """å°‡èˆŠçš„ JSON æª”ç§»å…¥ history è³‡æ–™å¤¾é€²è¡Œå‚™ä»½"""
    if not os.path.exists(HISTORY_DIR):
        os.makedirs(HISTORY_DIR)
        
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_filename = f"{source_name}_{timestamp}.json"
    backup_path = os.path.join(HISTORY_DIR, backup_filename)
    
    try:
        shutil.move(file_path, backup_path)
    except Exception as e:
        print(f"âš ï¸ [DiffEngine] å‚™ä»½èˆŠæª”å¤±æ•—: {e}")

    # åŸ·è¡Œè¼ªæ›¿ï¼Œåˆªé™¤éèˆŠçš„å‚™ä»½
    cleanup_history(source_name)

def cleanup_history(source_name: str):
    """æ¸…ç†éèˆŠçš„æ­·å²å‚™ä»½ï¼Œåªä¿ç•™ MAX_HISTORY_COUNT ä»½"""
    pattern = os.path.join(HISTORY_DIR, f"{source_name}_*.json")
    files = glob.glob(pattern)
    # ä¾ä¿®æ”¹æ™‚é–“æ’åº (æ–°çš„åœ¨å‰)
    files.sort(key=os.path.getmtime, reverse=True)
    
    if len(files) > MAX_HISTORY_COUNT:
        for f in files[MAX_HISTORY_COUNT:]:
            try:
                os.remove(f)
            except OSError:
                pass

# ==========================================
# ğŸš€ æ ¸å¿ƒé‚è¼¯
# ==========================================

def process_diff_and_save(source_name: str, new_chunks: List[Dict]) -> Optional[Dict]:
    """
    åŸ·è¡Œ Diff æ¯”å°ï¼Œä¸¦åŒ…å«ã€Œå€‹åˆ¥ä¾†æºç†”æ–·æ©Ÿåˆ¶ã€ã€‚
    
    Args:
        source_name: è³‡æ–™ä¾†æºåç¨± (å¦‚ 'm365_roadmap')ï¼Œç”¨æ–¼æ±ºå®šæª”åã€‚
        new_chunks: çˆ¬èŸ²å‰›æŠ“å›ä¾†çš„æœ€æ–° Chunk åˆ—è¡¨ã€‚
        
    Returns:
        Dict: åŒ…å« status, added, deleted çš„å ±å‘Šã€‚
        None: è‹¥ç„¡è®Šå‹•å‰‡å›å‚³ Noneã€‚
    """
    if not os.path.exists(DATA_DIR):
        os.makedirs(DATA_DIR)

    # æ¯å€‹ä¾†æºæœ‰è‡ªå·±çš„ç¨ç«‹æª”æ¡ˆ (ä¾‹å¦‚ data/m365_roadmap.json)
    file_path = os.path.join(DATA_DIR, f"{source_name}.json")
    
    # 1. æº–å‚™æ–°è³‡æ–™ (è¨ˆç®— Hash ä¸¦å»ºç«‹ Map)
    new_chunk_map = {}
    for chunk in new_chunks:
        # Token æª¢æŸ¥ (åƒ…è­¦å‘Š)
        if count_tokens(chunk.get("content", "")) > TOKEN_LIMIT:
            print(f"\033[91mâš ï¸ [è­¦å‘Š] Chunk token éé•·: {chunk.get('title', 'Unknown')}\033[0m")
            
        fp = calculate_chunk_fingerprint(chunk)
        chunk["id"] = fp 
        new_chunk_map[fp] = chunk

    # 2. è®€å–èˆŠè³‡æ–™ (ä½œç‚º Source of Truth)
    old_chunk_map = {}
    if os.path.exists(file_path):
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                old_data = json.load(f)
                for chunk in old_data:
                    # å³ä½¿èˆŠæª”æœ‰ idï¼Œä»é‡æ–°è¨ˆç®—ä»¥ç¢ºä¿é‚è¼¯ä¸€è‡´
                    fp = calculate_chunk_fingerprint(chunk)
                    chunk["id"] = fp
                    old_chunk_map[fp] = chunk
        except Exception as e:
            print(f"âš ï¸ [DiffEngine] è®€å–èˆŠæª”å¤±æ•— ({file_path})ï¼Œè¦–ç‚ºå…¨æ–°å¢: {e}")
            old_chunk_map = {}

    # 3. é›†åˆé‹ç®—æ‰¾å‡ºå·®ç•°
    new_hashes = set(new_chunk_map.keys())
    old_hashes = set(old_chunk_map.keys())

    added_hashes = new_hashes - old_hashes
    deleted_hashes = old_hashes - new_hashes
    
    # =========================================================
    # ğŸ”¥ğŸ”¥ğŸ”¥ [å€‹åˆ¥ä¾†æºç†”æ–·æ©Ÿåˆ¶] Per-Source Circuit Breaker ğŸ”¥ğŸ”¥ğŸ”¥
    # =========================================================
    total_existing_count = len(old_hashes)
    deletion_count = len(deleted_hashes)

    # è§¸ç™¼æ¢ä»¶ï¼š
    # 1. åŸæœ¬æœ‰è³‡æ–™ (total > 0)
    # 2. åˆªé™¤æ•¸é‡ > 5 (é¿å…è³‡æ–™é‡æ¥µå°‘æ™‚çš„èª¤åˆ¤)
    # 3. åˆªé™¤æ¯”ä¾‹ > 33.3% (1/3)
    if total_existing_count > 0 and deletion_count > 5:
        if deletion_count > (total_existing_count / 3):
            print("\n" + "!"*60)
            print(f"ğŸ›‘ [ç†”æ–·è­¦å‘Š - {source_name}]")
            print(f"ğŸ›‘ è©²ä¾†æºåŸæœ¬æœ‰ {total_existing_count} ç­†ï¼Œæœ¬æ¬¡è©¦åœ–åˆªé™¤ {deletion_count} ç­†ã€‚")
            print(f"ğŸ›‘ åˆªé™¤æ¯”ä¾‹ ({deletion_count/total_existing_count:.1%}) è¶…é 1/3 å®‰å…¨é–¥å€¼ã€‚")
            print(f"ğŸ›¡ï¸ ç³»çµ±å·²æ‹’çµ•æ›´æ–°æ­¤ä¾†æºã€‚èˆŠè³‡æ–™å°‡è¢«å®Œæ•´ä¿ç•™ã€‚")
            print("!"*60 + "\n")

            # å›å‚³ç‰¹æ®Šç‹€æ…‹ï¼Œå‘ŠçŸ¥ Scheduler ç™¼ç”Ÿäº†ä»€éº¼äº‹
            return {
                "source": source_name,
                "status": "CIRCUIT_BREAKER_TRIGGERED", # ç‰¹æ®Šæ¨™è¨˜
                "added": [],
                "deleted": []
            }

    # 4. è‹¥ç„¡è®Šå‹•
    if not added_hashes and not deleted_hashes:
        print(f"ğŸ’¤ {source_name} è³‡æ–™ç„¡è®Šå‹•ã€‚")
        return None

    print(f"ğŸ’¾ {source_name} åµæ¸¬åˆ°è®Šå‹•: æ–°å¢ {len(added_hashes)} ç­†, åˆªé™¤ {len(deleted_hashes)} ç­†")

    # 5. åŸ·è¡Œå­˜æª” (åªæœ‰é€šéæª¢æŸ¥æ‰æœƒèµ°åˆ°é€™ä¸€æ­¥)
    # å…ˆå‚™ä»½èˆŠæª”
    if os.path.exists(file_path):
        archive_old_file(source_name, file_path)
    
    # å¯«å…¥æ–°æª” (Source of Truth æ›´æ–°)
    try:
        with open(file_path, "w", encoding="utf-8") as f:
            # å°‡ map è½‰å› list å­˜æª”
            final_list = list(new_chunk_map.values())
            json.dump(final_list, f, ensure_ascii=False, indent=4)
    except Exception as e:
        print(f"âŒ [DiffEngine] å¯«å…¥æª”æ¡ˆå¤±æ•—: {e}")
        return None

    # 6. å›å‚³æ­£å¸¸å ±å‘Š
    return {
        "source": source_name,
        "status": "SUCCESS",
        "added": [new_chunk_map[h] for h in added_hashes],
        "deleted": [old_chunk_map[h] for h in deleted_hashes]
    }